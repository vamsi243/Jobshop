{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 1. Basic Text Pre-processing\n",
    "#### Student Name: Sai Vamsi Chunduru\n",
    "#### Student ID: S3884753\n",
    "\n",
    "Date: 02-10-2021\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* re\n",
    "* numpy\n",
    "* load_files\n",
    "* RegexpTokenizer\n",
    "* sent_tokenize\n",
    "* chain\n",
    "*  nltk.probability\n",
    "* ngrams\n",
    "## Introduction\n",
    "Task 1, is a Basic Text Pre-processing.I will be performing basic text pre-processing on the given job advertisement datasets, including, In this task we perform tokenization, removing most/less frequent words and stop words , extracting bigrams and\n",
    "collocations from job descriptions.\n",
    "\n",
    "   In this task, I also performed the pre-processing on the title+job description and title too.I have stored these files in atext format to be used quickly for task 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries as you need in this assessment, e.g.,\n",
    "from sklearn.datasets import load_files  \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk import RegexpTokenizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from itertools import chain\n",
    "from nltk.probability import *\n",
    "from nltk.util import ngrams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Examining and loading data\n",
    "- Examined the Data folder.The Data folder consists of a large collection of job advertisement documents and contains around 55448 jobs.The data folder got 8 different sub-folders, namely: Accounting_Finance, Engineering,Healthcare_Nursing, Hospitality_Catering, IT, PR_Advertising_Marketing, Sales and Teaching, each folder name is a job category.Each job advertisement document is a txt file, named as Job_<ID/>.txt.(The ID heremeans the job ID)The text file contains the title, the webindex,(some will also have information on the company name, some might not), and the full description of the job advertisement.\n",
    "\n",
    "- Loaded the data folder with help of sklearn.datasets.load_files, into proper data structure(i.e. in a dictonary format) the loaded file consists of ['data', 'filenames', 'target_names', 'target', 'DESCR'] as dictonary keys, filenames got the name of text file loaded. target names got all the 8 category names.The data has all the job data stored in it.\n",
    "\n",
    "- webIndex and description are stored in a list format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The contents of data flder are read into the ad_data dictonary\n",
    "ad_data = load_files(r'data') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking the length of the extracted data folder.\n",
    "len(ad_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['data', 'filenames', 'target_names', 'target', 'DESCR'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displying the keys of ad_data.\n",
    "ad_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['data\\\\Engineering\\\\Job_14624.txt',\n",
       "       'data\\\\Healthcare_Nursing\\\\Job_31567.txt',\n",
       "       'data\\\\Hospitality_Catering\\\\Job_50131.txt', ...,\n",
       "       'data\\\\IT\\\\Job_13401.txt',\n",
       "       'data\\\\PR_Advertising_Marketing\\\\Job_52696.txt',\n",
       "       'data\\\\Accounting_Finance\\\\Job_25296.txt'], dtype='<U43')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the contents of filenames\n",
    "ad_data['filenames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Accounting_Finance',\n",
       " 'Engineering',\n",
       " 'Healthcare_Nursing',\n",
       " 'Hospitality_Catering',\n",
       " 'IT',\n",
       " 'PR_Advertising_Marketing',\n",
       " 'Sales',\n",
       " 'Teaching']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the contents of target_names\n",
    "ad_data['target_names'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, ..., 4, 5, 0])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Displaying the contents of target variable\n",
    "ad_data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I assigned a temporary variable as temp, to be used as a validator in this task-1.\n",
    "temp=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "b'Title: Inpatient Ward Team Leader\\nWebindex: 64752715\\nDescription: Skills: Inpatient Ward Team Leader Description: In Patient Team Leader  Southampton  Permanent  up to **** We currently have an opportunity for a senior registered general nurse to join an established team within a inpatient ward at a Private Hospital located in Southampton. This is a full time permanent role offering a salary in the region of **** to **** You will be UK experienced and have a background in surgical nursing. You will have previous experience of leading a team of staff and being accountable for senior duties within a ward environment. This is a day duty position, although as a senior member of staff you will be required to make yourself available to work around the needs of the ward. You will be responsible for pre and post assessing patients and providing them with a holistic package of care through to discharge, to include clinical assessments. To apply and for a job description, please call Isobelle at STR Health on **** **** **** or email your CV to ifishstrgroup.co.ukSTR Health Limited is acting as an Employment Agency in relation to this vacancy.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the contents of job advertisement text files.\n",
    "ad_data['data'][temp]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55449\n"
     ]
    }
   ],
   "source": [
    "#Checking for the number of job advetisements\n",
    "print(len(ad_data['data']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting job description from all jobs\n",
    "job_descr_list=[] #It stores the job description alone for job in a list format\n",
    "webind=[] #It stores the webindex of job.\n",
    "title1=[] #It holds the unprocessed job title data - to be used in task-3\n",
    "title=[] #It holds the processed job title data.processed\n",
    "title_description=[] #It holds the job title + description data- to be used in task-3\n",
    "for i in ad_data['data']:\n",
    "    str1=i.decode('utf8')  #Converting the bytes-like object to python string,\n",
    "    strz1 =list(str(str1).split(\"\\nDescription:\")) #storing the descriptionas string by spliting the job advertisement data.\n",
    "    strz12 =list(str(str1).split(\"\\n\")) #this stores the tite, webindex,company name and description in a list format.\n",
    "    title1.append(strz12[0]) \n",
    "    title.append(strz12[0].split(\": \")[1])\n",
    "    webind.append(strz12[1])\n",
    "    title_description.append(strz12[0].split(\": \")[1]+strz1[1])\n",
    "    job_descr_list.append(strz1[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Skills: Inpatient Ward Team Leader Description: In Patient Team Leader  Southampton  Permanent  up to **** We currently have an opportunity for a senior registered general nurse to join an established team within a inpatient ward at a Private Hospital located in Southampton. This is a full time permanent role offering a salary in the region of **** to **** You will be UK experienced and have a background in surgical nursing. You will have previous experience of leading a team of staff and being accountable for senior duties within a ward environment. This is a day duty position, although as a senior member of staff you will be required to make yourself available to work around the needs of the ward. You will be responsible for pre and post assessing patients and providing them with a holistic package of care through to discharge, to include clinical assessments. To apply and for a job description, please call Isobelle at STR Health on **** **** **** or email your CV to ifishstrgroup.co.ukSTR Health Limited is acting as an Employment Agency in relation to this vacancy.\n",
      "\n",
      "Inpatient Ward Team Leader\n",
      "\n",
      "Inpatient Ward Team Leader Skills: Inpatient Ward Team Leader Description: In Patient Team Leader  Southampton  Permanent  up to **** We currently have an opportunity for a senior registered general nurse to join an established team within a inpatient ward at a Private Hospital located in Southampton. This is a full time permanent role offering a salary in the region of **** to **** You will be UK experienced and have a background in surgical nursing. You will have previous experience of leading a team of staff and being accountable for senior duties within a ward environment. This is a day duty position, although as a senior member of staff you will be required to make yourself available to work around the needs of the ward. You will be responsible for pre and post assessing patients and providing them with a holistic package of care through to discharge, to include clinical assessments. To apply and for a job description, please call Isobelle at STR Health on **** **** **** or email your CV to ifishstrgroup.co.ukSTR Health Limited is acting as an Employment Agency in relation to this vacancy.\n"
     ]
    }
   ],
   "source": [
    "#Displaying job description ,title and title+description info respectively.\n",
    "print(job_descr_list[temp]+\"\\n\")\n",
    "print(title[temp]+\"\\n\")\n",
    "print(title_description[temp])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Pre-processing data\n",
    "Perform the required text pre-processing steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...... Sections and code blocks on basic text pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This function converts the description,title,title+ descriptions into tokens\n",
    "def tokenizeDescription(discip):\n",
    "    \"\"\"\n",
    "        This function first convert all words to lowercases, \n",
    "        it then segment the raw review into sentences and tokenize each sentences \n",
    "        and convert the review to a list of tokens.\n",
    "    \"\"\"     \n",
    "    # convert all words to lowercase   \n",
    "    nl_discip = discip.lower() \n",
    "    \n",
    "    # segament into sentences\n",
    "    sentences = sent_tokenize(nl_discip)\n",
    "    \n",
    "    # tokenize each sentence\n",
    "    pattern =  r\"[a-zA-Z]+(?:[-'][a-zA-Z]+)?\"\n",
    "    tokenizer = RegexpTokenizer(pattern) \n",
    "    token_lists = [tokenizer.tokenize(sen) for sen in sentences]\n",
    "    \n",
    "    # merge them into a list of tokens\n",
    "    tokenised_descrip = list(chain.from_iterable(token_lists))\n",
    "    return tokenised_descrip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    " # list comprehension, generate a list of tokenized words of job descriptions\n",
    "tk_descrip = [tokenizeDescription(r) for r in job_descr_list] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    " # list comprehension, generate a list of tokenized words of job title + descriptions\n",
    "tk_title_descrip = [tokenizeDescription(r) for r in title_description] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    " # list comprehension, generate a list of tokenized words of job title\n",
    "tk_title = [tokenizeDescription(r) for r in title] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55449\n",
      "55449\n",
      "55449\n"
     ]
    }
   ],
   "source": [
    "#Displaying job description ,title and title+description size.\n",
    "print(len(tk_descrip))\n",
    "print(len(tk_title_descrip))\n",
    "print(len(tk_title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n",
      "179\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#Displaying job description ,title and title+description sizes based on the temporary variable temp.\n",
    "print(len(tk_title_descrip[temp]))\n",
    "print(len(tk_descrip[temp]))\n",
    "print(len(tk_title[temp]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a function to display the vocabulary size, number of tokens and total number of jobs.\n",
    "#This is created to monitor the changes in these variables after every pre-processing task\n",
    "def stats_print(descrips):\n",
    "    words_1 = list(chain.from_iterable(descrips)) # we put all the tokens in the corpus in a single list\n",
    "    vocab_1 = set(words_1) # compute the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words\n",
    "    print(\"Vocabulary size: \",len(vocab_1))\n",
    "    print(\"Total number of tokens: \", len(words_1))\n",
    "    print(\"Total number of jobs:\", len(descrips))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  89591\n",
      "Total number of tokens:  13799127\n",
      "Total number of jobs: 55449\n",
      "Vocabulary size:  91004\n",
      "Total number of tokens:  14073878\n",
      "Total number of jobs: 55449\n",
      "Vocabulary size:  10764\n",
      "Total number of tokens:  274751\n",
      "Total number of jobs: 55449\n"
     ]
    }
   ],
   "source": [
    "#Displaying job description ,title and title+description statistics, befor any pre processing.\n",
    "stats_print(tk_descrip)\n",
    "stats_print(tk_title_descrip)\n",
    "stats_print(tk_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing words which are less than 2 in job description\n",
    "tk_descrip1 = [[w for w in review if len(w) >=2] \\\n",
    "                      for review in tk_descrip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing words which are less than 2 in job title+description\n",
    "tk_title_descrip1 = [[w for w in review if len(w) >=2] \\\n",
    "                      for review in tk_title_descrip]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# removing words which are less than 2 in job title\n",
    "tk_title1 = [[w for w in review if len(w) >=2] \\\n",
    "                      for review in tk_title]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens count:\n",
      " 171\n",
      "Tokenized descriptions:\n",
      " ['inpatient', 'ward', 'team', 'leader', 'skills', 'inpatient', 'ward', 'team', 'leader', 'description', 'in', 'patient', 'team', 'leader', 'southampton', 'permanent', 'up', 'to', 'we', 'currently', 'have', 'an', 'opportunity', 'for', 'senior', 'registered', 'general', 'nurse', 'to', 'join', 'an', 'established', 'team', 'within', 'inpatient', 'ward', 'at', 'private', 'hospital', 'located', 'in', 'southampton', 'this', 'is', 'full', 'time', 'permanent', 'role', 'offering', 'salary', 'in', 'the', 'region', 'of', 'to', 'you', 'will', 'be', 'uk', 'experienced', 'and', 'have', 'background', 'in', 'surgical', 'nursing', 'you', 'will', 'have', 'previous', 'experience', 'of', 'leading', 'team', 'of', 'staff', 'and', 'being', 'accountable', 'for', 'senior', 'duties', 'within', 'ward', 'environment', 'this', 'is', 'day', 'duty', 'position', 'although', 'as', 'senior', 'member', 'of', 'staff', 'you', 'will', 'be', 'required', 'to', 'make', 'yourself', 'available', 'to', 'work', 'around', 'the', 'needs', 'of', 'the', 'ward', 'you', 'will', 'be', 'responsible', 'for', 'pre', 'and', 'post', 'assessing', 'patients', 'and', 'providing', 'them', 'with', 'holistic', 'package', 'of', 'care', 'through', 'to', 'discharge', 'to', 'include', 'clinical', 'assessments', 'to', 'apply', 'and', 'for', 'job', 'description', 'please', 'call', 'isobelle', 'at', 'str', 'health', 'on', 'or', 'email', 'your', 'cv', 'to', 'ifishstrgroup', 'co', 'ukstr', 'health', 'limited', 'is', 'acting', 'as', 'an', 'employment', 'agency', 'in', 'relation', 'to', 'this', 'vacancy']\n"
     ]
    }
   ],
   "source": [
    "#Validating with temp variable after removing words that are have length less than 2 in job title+description\n",
    "print(\"Tokens count:\\n\",len(tk_title_descrip1[temp]))\n",
    "print(\"Tokenized descriptions:\\n\",tk_title_descrip1[temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens count:\n",
      " 4\n",
      "Tokenized descriptions:\n",
      " ['inpatient', 'ward', 'team', 'leader']\n"
     ]
    }
   ],
   "source": [
    "#Validating with temp variable after removing words that are have length less than 2 in job title\n",
    "print(\"Tokens count:\\n\",len(tk_title1[temp]))\n",
    "print(\"Tokenized descriptions:\\n\",tk_title1[temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens count:\n",
      " 167\n",
      "Tokenized descriptions:\n",
      " ['skills', 'inpatient', 'ward', 'team', 'leader', 'description', 'in', 'patient', 'team', 'leader', 'southampton', 'permanent', 'up', 'to', 'we', 'currently', 'have', 'an', 'opportunity', 'for', 'senior', 'registered', 'general', 'nurse', 'to', 'join', 'an', 'established', 'team', 'within', 'inpatient', 'ward', 'at', 'private', 'hospital', 'located', 'in', 'southampton', 'this', 'is', 'full', 'time', 'permanent', 'role', 'offering', 'salary', 'in', 'the', 'region', 'of', 'to', 'you', 'will', 'be', 'uk', 'experienced', 'and', 'have', 'background', 'in', 'surgical', 'nursing', 'you', 'will', 'have', 'previous', 'experience', 'of', 'leading', 'team', 'of', 'staff', 'and', 'being', 'accountable', 'for', 'senior', 'duties', 'within', 'ward', 'environment', 'this', 'is', 'day', 'duty', 'position', 'although', 'as', 'senior', 'member', 'of', 'staff', 'you', 'will', 'be', 'required', 'to', 'make', 'yourself', 'available', 'to', 'work', 'around', 'the', 'needs', 'of', 'the', 'ward', 'you', 'will', 'be', 'responsible', 'for', 'pre', 'and', 'post', 'assessing', 'patients', 'and', 'providing', 'them', 'with', 'holistic', 'package', 'of', 'care', 'through', 'to', 'discharge', 'to', 'include', 'clinical', 'assessments', 'to', 'apply', 'and', 'for', 'job', 'description', 'please', 'call', 'isobelle', 'at', 'str', 'health', 'on', 'or', 'email', 'your', 'cv', 'to', 'ifishstrgroup', 'co', 'ukstr', 'health', 'limited', 'is', 'acting', 'as', 'an', 'employment', 'agency', 'in', 'relation', 'to', 'this', 'vacancy']\n"
     ]
    }
   ],
   "source": [
    "#Validating with temp variable after removing words that are have length less than 2 in job description\n",
    "print(\"Tokens count:\\n\",len(tk_descrip1[temp]))\n",
    "print(\"Tokenized descriptions:\\n\",tk_descrip1[temp])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  89565\n",
      "Total number of tokens:  13342925\n",
      "Total number of jobs: 55449\n",
      "Vocabulary size:  90978\n",
      "Total number of tokens:  13608006\n",
      "Total number of jobs: 55449\n",
      "Vocabulary size:  10738\n",
      "Total number of tokens:  265081\n",
      "Total number of jobs: 55449\n"
     ]
    }
   ],
   "source": [
    "#Displaying job description ,title and title+description statistics, after removing words with length less than 2.\n",
    "stats_print(tk_descrip1)\n",
    "stats_print(tk_title_descrip1)\n",
    "stats_print(tk_title1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing stopwords on the data based on stopwords_en.txt from job description ,title and title+description statistics\n",
    "#storing the stop words in a list structure.\n",
    "stopwords_en = []\n",
    "with open('./stopwords_en.txt') as f:\n",
    "    stopwords_en = f.read().splitlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length :571\n",
      "['a', \"a's\", 'able', 'about', 'above', 'according', 'accordingly', 'across', 'actually', 'after', 'afterwards', 'again', 'against', \"ain't\", 'all', 'allow', 'allows', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'an', 'and', 'another', 'any', 'anybody', 'anyhow', 'anyone', 'anything', 'anyway', 'anyways', 'anywhere', 'apart', 'appear', 'appreciate', 'appropriate', 'are', \"aren't\", 'around', 'as', 'aside', 'ask', 'asking', 'associated', 'at', 'available', 'away', 'awfully', 'b', 'be', 'became', 'because', 'become', 'becomes', 'becoming', 'been', 'before', 'beforehand', 'behind', 'being', 'believe', 'below', 'beside', 'besides', 'best', 'better', 'between', 'beyond', 'both', 'brief', 'but', 'by', 'c', \"c'mon\", \"c's\", 'came', 'can', \"can't\", 'cannot', 'cant', 'cause', 'causes', 'certain', 'certainly', 'changes', 'clearly', 'co', 'com', 'come', 'comes', 'concerning', 'consequently', 'consider', 'considering', 'contain', 'containing', 'contains', 'corresponding', 'could', \"couldn't\", 'course', 'currently', 'd', 'definitely', 'described', 'despite', 'did', \"didn't\", 'different', 'do', 'does', \"doesn't\", 'doing', \"don't\", 'done', 'down', 'downwards', 'during', 'e', 'each', 'edu', 'eg', 'eight', 'either', 'else', 'elsewhere', 'enough', 'entirely', 'especially', 'et', 'etc', 'even', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'ex', 'exactly', 'example', 'except', 'f', 'far', 'few', 'fifth', 'first', 'five', 'followed', 'following', 'follows', 'for', 'former', 'formerly', 'forth', 'four', 'from', 'further', 'furthermore', 'g', 'get', 'gets', 'getting', 'given', 'gives', 'go', 'goes', 'going', 'gone', 'got', 'gotten', 'greetings', 'h', 'had', \"hadn't\", 'happens', 'hardly', 'has', \"hasn't\", 'have', \"haven't\", 'having', 'he', \"he's\", 'hello', 'help', 'hence', 'her', 'here', \"here's\", 'hereafter', 'hereby', 'herein', 'hereupon', 'hers', 'herself', 'hi', 'him', 'himself', 'his', 'hither', 'hopefully', 'how', 'howbeit', 'however', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'ie', 'if', 'ignored', 'immediate', 'in', 'inasmuch', 'inc', 'indeed', 'indicate', 'indicated', 'indicates', 'inner', 'insofar', 'instead', 'into', 'inward', 'is', \"isn't\", 'it', \"it'd\", \"it'll\", \"it's\", 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kept', 'know', 'knows', 'known', 'l', 'last', 'lately', 'later', 'latter', 'latterly', 'least', 'less', 'lest', 'let', \"let's\", 'like', 'liked', 'likely', 'little', 'look', 'looking', 'looks', 'ltd', 'm', 'mainly', 'many', 'may', 'maybe', 'me', 'mean', 'meanwhile', 'merely', 'might', 'more', 'moreover', 'most', 'mostly', 'much', 'must', 'my', 'myself', 'n', 'name', 'namely', 'nd', 'near', 'nearly', 'necessary', 'need', 'needs', 'neither', 'never', 'nevertheless', 'new', 'next', 'nine', 'no', 'nobody', 'non', 'none', 'noone', 'nor', 'normally', 'not', 'nothing', 'novel', 'now', 'nowhere', 'o', 'obviously', 'of', 'off', 'often', 'oh', 'ok', 'okay', 'old', 'on', 'once', 'one', 'ones', 'only', 'onto', 'or', 'other', 'others', 'otherwise', 'ought', 'our', 'ours', 'ourselves', 'out', 'outside', 'over', 'overall', 'own', 'p', 'particular', 'particularly', 'per', 'perhaps', 'placed', 'please', 'plus', 'possible', 'presumably', 'probably', 'provides', 'q', 'que', 'quite', 'qv', 'r', 'rather', 'rd', 're', 'really', 'reasonably', 'regarding', 'regardless', 'regards', 'relatively', 'respectively', 'right', 's', 'said', 'same', 'saw', 'say', 'saying', 'says', 'second', 'secondly', 'see', 'seeing', 'seem', 'seemed', 'seeming', 'seems', 'seen', 'self', 'selves', 'sensible', 'sent', 'serious', 'seriously', 'seven', 'several', 'shall', 'she', 'should', \"shouldn't\", 'since', 'six', 'so', 'some', 'somebody', 'somehow', 'someone', 'something', 'sometime', 'sometimes', 'somewhat', 'somewhere', 'soon', 'sorry', 'specified', 'specify', 'specifying', 'still', 'sub', 'such', 'sup', 'sure', 't', \"t's\", 'take', 'taken', 'tell', 'tends', 'th', 'than', 'thank', 'thanks', 'thanx', 'that', \"that's\", 'thats', 'the', 'their', 'theirs', 'them', 'themselves', 'then', 'thence', 'there', \"there's\", 'thereafter', 'thereby', 'therefore', 'therein', 'theres', 'thereupon', 'these', 'they', \"they'd\", \"they'll\", \"they're\", \"they've\", 'think', 'third', 'this', 'thorough', 'thoroughly', 'those', 'though', 'three', 'through', 'throughout', 'thru', 'thus', 'to', 'together', 'too', 'took', 'toward', 'towards', 'tried', 'tries', 'truly', 'try', 'trying', 'twice', 'two', 'u', 'un', 'under', 'unfortunately', 'unless', 'unlikely', 'until', 'unto', 'up', 'upon', 'us', 'use', 'used', 'useful', 'uses', 'using', 'usually', 'uucp', 'v', 'value', 'various', 'very', 'via', 'viz', 'vs', 'w', 'want', 'wants', 'was', \"wasn't\", 'way', 'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'welcome', 'well', 'went', 'were', \"weren't\", 'what', \"what's\", 'whatever', 'when', 'whence', 'whenever', 'where', \"where's\", 'whereafter', 'whereas', 'whereby', 'wherein', 'whereupon', 'wherever', 'whether', 'which', 'while', 'whither', 'who', \"who's\", 'whoever', 'whole', 'whom', 'whose', 'why', 'will', 'willing', 'wish', 'with', 'within', 'without', \"won't\", 'wonder', 'would', 'would', \"wouldn't\", 'x', 'y', 'yes', 'yet', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves', 'z', 'zero']\n"
     ]
    }
   ],
   "source": [
    "#Displaying the stopwords and their number extracted from stopword file\n",
    "print(\"length :\"+str(len(stopwords_en)))\n",
    "print(stopwords_en)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55449"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flitering stop words from job description\n",
    "filtered_descrip_tokens=[]\n",
    "filtered_tokens=[]\n",
    "for tk_desc in tk_descrip1:\n",
    "    filtered_tokens = [token for token in tk_desc if token not in stopwords_en] #Check the existance of words against the stopwords and extract them in list comprehension\n",
    "    filtered_descrip_tokens.append(filtered_tokens)\n",
    "len(filtered_descrip_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55449"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flitering stop words from job title+description\n",
    "filtered_title_descrip_tokens=[]\n",
    "filtered_tokens=[]\n",
    "for tk_title_desc in tk_title_descrip1:\n",
    "    filtered_tokens = [token for token in tk_title_desc if token not in stopwords_en] #Check the existance of words against the stopwords and extract them in list comprehension\n",
    "    filtered_title_descrip_tokens.append(filtered_tokens)\n",
    "len(filtered_title_descrip_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55449"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# flitering stop words from job title\n",
    "filtered_title_tokens=[]\n",
    "filtered_tokens=[]\n",
    "for tk_tle in tk_title1:\n",
    "    filtered_tokens = [token for token in tk_tle if token not in stopwords_en] #Check the existance of words against the stopwords and extract them in list comprehension\n",
    "    filtered_title_tokens.append(filtered_tokens)\n",
    "len(filtered_title_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88\n",
      "92\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "#Displaying filtered list of temp element from job description ,title and title+description , after removing stopwords.\n",
    "print(len(filtered_descrip_tokens[temp]))\n",
    "print(len(filtered_title_descrip_tokens[temp]))\n",
    "print(len(filtered_title_tokens[temp]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  89052\n",
      "Total number of tokens:  7863307\n",
      "Total number of jobs: 55449\n",
      "Vocabulary size:  90465\n",
      "Total number of tokens:  8114168\n",
      "Total number of jobs: 55449\n",
      "Vocabulary size:  10531\n",
      "Total number of tokens:  250861\n",
      "Total number of jobs: 55449\n"
     ]
    }
   ],
   "source": [
    "#Displaying statistics of description ,title and title+description , after removing stopwords.\n",
    "stats_print(filtered_descrip_tokens)\n",
    "stats_print(filtered_title_descrip_tokens)\n",
    "stats_print(filtered_title_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Remove the word that appears only once in the document collection, based on term frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we put all the tokens in the corpus in a single list of description ,title and title+description.\n",
    "words = list(chain.from_iterable(filtered_descrip_tokens)) #contains tokens of job description\n",
    "words1 = list(chain.from_iterable(filtered_title_descrip_tokens)) #contains tokens of job title+description\n",
    "words2 = list(chain.from_iterable(filtered_title_tokens)) #Contains tokens of job title\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_fd = FreqDist(words) # computed term frequency for each unique word/type for job description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_fd1 = FreqDist(words1) # computed term frequency for each unique word/type for job title+description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_fd2 = FreqDist(words2) # computed term frequency for each unique word/type for job title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('experience', 104062),\n",
       " ('role', 66536),\n",
       " ('work', 64587),\n",
       " ('team', 62308),\n",
       " ('business', 59184)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_fd.most_common(5)#top 5 most frequent words from description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('experience', 104373),\n",
       " ('role', 66686),\n",
       " ('work', 64721),\n",
       " ('team', 63189),\n",
       " ('business', 61341)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_fd1.most_common(5) #top 5 most frequent words from job title+description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('manager', 10710),\n",
       " ('engineer', 6176),\n",
       " ('developer', 4614),\n",
       " ('sales', 4609),\n",
       " ('senior', 4106)]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "term_fd2.most_common(5)#top 5 most frequent words from job title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "48964"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracted the distinct/less frequent words from job description term frequency.\n",
    "lessFreqWords = set(term_fd.hapaxes())\n",
    "len(lessFreqWords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "49660"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracted the distinct/less frequent words from job title+description term frequency.\n",
    "lessFreqWords1 = set(term_fd1.hapaxes())\n",
    "len(lessFreqWords1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4920"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Extracted the distinct/less frequent words from job title term frequency.\n",
    "lessFreqWords2 = set(term_fd2.hapaxes())\n",
    "len(lessFreqWords2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a function to remove lessfrequent words.\n",
    "#I removed/filtered out less frequent words from tokens cause as they cause noise in the data and does not add any value.\n",
    "def removeLessFreqWords(desc):\n",
    "    return [w for w in desc if w not in lessFreqWords]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtered out the less frequent words from the job description tokens\n",
    "filtered_descrip_tokens1 = [removeLessFreqWords(descrp) for descrp in filtered_descrip_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtered out the less frequent words from the job title+description tokens\n",
    "filtered_title_descrip_tokens1 = [removeLessFreqWords(descrp) for descrp in filtered_title_descrip_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtered out the less frequent words from the job title tokens\n",
    "filtered_title_tokens1 = [removeLessFreqWords(descrp) for descrp in filtered_title_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  40088\n",
      "Total number of tokens:  7814343\n",
      "Total number of jobs: 55449\n",
      "Vocabulary size:  41501\n",
      "Total number of tokens:  8064487\n",
      "Total number of jobs: 55449\n",
      "Vocabulary size:  9924\n",
      "Total number of tokens:  250144\n",
      "Total number of jobs: 55449\n"
     ]
    }
   ],
   "source": [
    "#Displaying statistics of description ,title and title+description , after removing less frequent words.\n",
    "stats_print(filtered_descrip_tokens1)\n",
    "stats_print(filtered_title_descrip_tokens1)\n",
    "stats_print(filtered_title_tokens1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Remove the top 50 most frequent words based on document frequency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we put all the tokens in the corpus in a single list of description-words_2 list\n",
    "words_2 = list(chain.from_iterable([set(desc) for desc in filtered_descrip_tokens1]))\n",
    "doc_fd = FreqDist(words_2)  # computed document frequency for each unique word/type of job description\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['experience', 'role', 'work', 'team', 'working', 'skills', 'client', 'job', 'business', 'uk', 'excellent', 'opportunity', 'company', 'management', 'required', 'development', 'apply', 'based', 'successful', 'join', 'www', 'salary', 'cv', 'support', 'knowledge', 'strong', 'environment', 'posted', 'jobseeking', 'candidate', 'originally', 'leading', 'high', 'service', 'manager', 'good', 'ability', 'including', 'position', 'services', 'benefits', 'training', 'essential', 'experienced', 'key', 'contact', 'level', 'recruitment', 'candidates', 'provide']\n"
     ]
    }
   ],
   "source": [
    "#fer50 holds the top 50 common words in job description.\n",
    "freq50=[w[0] for w in doc_fd.most_common(50)]\n",
    "len(freq50)\n",
    "print(freq50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I Removed the top 50 most frequent words in document based on document frequency, cause the increase in redundant data adds upto data noise.\n",
    "#Removed the most common words from job description.\n",
    "def removeMoreFreqWords(desc):\n",
    "    return [w for w in desc if w not in freq50]\n",
    "\n",
    "descrip_tokens_tk = [removeMoreFreqWords(descrp) for descrp in filtered_descrip_tokens1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we put all the tokens in the corpus in a single list of job title+description-words1_2 list\n",
    "words1_2 = list(chain.from_iterable([set(tle_desc) for tle_desc in filtered_title_descrip_tokens1]))\n",
    "doc_fd1 = FreqDist(words1_2)  # computed document frequency for each unique word/type in job title+description.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['experience', 'role', 'work', 'team', 'working', 'skills', 'client', 'job', 'business', 'uk', 'excellent', 'opportunity', 'company', 'required', 'management', 'development', 'apply', 'based', 'successful', 'join', 'salary', 'www', 'support', 'cv', 'knowledge', 'manager', 'strong', 'environment', 'posted', 'jobseeking', 'candidate', 'originally', 'leading', 'high', 'service', 'good', 'ability', 'including', 'services', 'position', 'benefits', 'training', 'essential', 'experienced', 'level', 'key', 'contact', 'recruitment', 'candidates', 'provide']\n"
     ]
    }
   ],
   "source": [
    "#fer50_1 holds the top 50 common words in job title_description.\n",
    "freq50_1=[w[0] for w in doc_fd1.most_common(50)]\n",
    "len(freq50_1)\n",
    "print(freq50_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I Removed the top 50 most frequent words in document based on document frequency, cause the increase in redundant data adds upto data noise.\n",
    "#Removed the most common words from job title+description.\n",
    "def removeMoreFreqWords1(desc):\n",
    "    return [w for w in desc if w not in freq50_1]\n",
    "\n",
    "title_descrip_tokens_tk = [removeMoreFreqWords1(descrp) for descrp in filtered_title_descrip_tokens1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we put all the tokens in the corpus in a single list of job title-words2_2 list\n",
    "words2_2 = list(chain.from_iterable([set(tle) for tle in filtered_title_tokens1]))\n",
    "doc_fd2 = FreqDist(words2_2)  # computed document frequency for each unique word/type in job title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['manager', 'engineer', 'developer', 'sales', 'senior', 'london', 'analyst', 'support', 'nurse', 'assistant', 'executive', 'business', 'chef', 'consultant', 'software', 'account', 'care', 'project', 'development', 'rgn', 'home', 'technical', 'teacher', 'web', 'service', 'marketing', 'sql', 'lead', 'design', 'head', 'systems', 'services', 'administrator', 'net', 'team', 'west', 'financial', 'restaurant', 'graduate', 'java', 'staff', 'technician', 'worker', 'general', 'de', 'junior', 'finance', 'digital', 'hotel', 'aspnet']\n"
     ]
    }
   ],
   "source": [
    "#fer50_2 holds the top 50 common words in job title.\n",
    "freq50_2=[w[0] for w in doc_fd2.most_common(50)]\n",
    "len(freq50_2)\n",
    "print(freq50_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I Removed the top 50 most frequent words in document, based on document frequency cause the increase in redundant data adds upto data noise.\n",
    "#Removed the most common words from job title.\n",
    "def removeMoreFreqWords2(desc):\n",
    "    return [w for w in desc if w not in freq50_2]\n",
    "\n",
    "title_tokens_tk = [removeMoreFreqWords2(descrp) for descrp in filtered_title_tokens1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  9874\n",
      "Total number of tokens:  159854\n",
      "Total number of jobs: 55449\n",
      "Vocabulary size:  41451\n",
      "Total number of tokens:  6462506\n",
      "Total number of jobs: 55449\n",
      "Vocabulary size:  40038\n",
      "Total number of tokens:  6239169\n",
      "Total number of jobs: 55449\n"
     ]
    }
   ],
   "source": [
    "#Displaying statistics of description ,title and title+description , after removing most frequent words based on document frequency.\n",
    "stats_print(title_tokens_tk)\n",
    "stats_print(title_descrip_tokens_tk)\n",
    "stats_print(descrip_tokens_tk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#descrip_tokens_tk122\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "#after removing single len characters,stopwords,less freq, top50 common words.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract the top 10 Bigrams based on term frequency, save them as a txt file (refer to the required\n",
    "output).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we put all the tokens in the corpus in a single list of job description.\n",
    "words_list = list(chain.from_iterable(descrip_tokens_tk)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting the bigrams of job description.\n",
    "bigrams = ngrams(words_list, n = 2)\n",
    "fdbigram = FreqDist(bigrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "#extracting top 10 bi-grams of job description.\n",
    "bigram_desc=fdbigram.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(('employment', 'agency'), 8055),\n",
       " (('track', 'record'), 5472),\n",
       " (('acting', 'employment'), 5095),\n",
       " (('sql', 'server'), 4804),\n",
       " (('asp', 'net'), 4687),\n",
       " (('relation', 'vacancy'), 3977),\n",
       " (('sales', 'executive'), 3619),\n",
       " (('chef', 'de'), 3586),\n",
       " (('nursing', 'home'), 3503),\n",
       " (('de', 'partie'), 3396)]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the top 10 most common bigrams of job description.\n",
    "bigram_desc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving required outputs\n",
    "Save the vocabulary, bigrams and job advertisment txt as per spectification.\n",
    "- vocab.txt\n",
    "- bigram.txt\n",
    "- job_ads.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save output data...\n",
    "#Extracting job id and category from the file names\n",
    "ids1=[]\n",
    "category=[]\n",
    "for i in ad_data['filenames']:\n",
    "    ss=str(i).split(\"\\\\\")\n",
    "    category.append(\"Category: \"+str(ss[1]))\n",
    "    ids1.append(\"ID: \"+str(str(str(ss[2]).split(\".txt\")[0]).split(\"Job_\")[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the processed description in the required format from the processed job description tokens\n",
    "desc_list=[] #contains the description data to be imputed onto job_ads.txt file\n",
    "desc_list_temp=[] #contains the processed-description data that needs to stored in a job_ads dataframe\n",
    "for i in descrip_tokens_tk:\n",
    "    listToStr = ' '.join(map(str, i))\n",
    "    desc_list.append(\"Description: \"+listToStr)\n",
    "    desc_list_temp.append(listToStr)\n",
    "    listToStr=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the processed job title+description in the required format from the processed job title+description tokens.\n",
    "tle_desc_list_temp=[] #contains the processed-title+description data that needs to stored in a job_ads dataframe\n",
    "for i in title_descrip_tokens_tk:\n",
    "    listToStr = ' '.join(map(str, i))\n",
    "    tle_desc_list_temp.append(listToStr)\n",
    "    listToStr=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the processed job title in the required format from the processed job title tokens.\n",
    "title_list_temp=[] #contains the processed-title data that needs to stored in a job_ads dataframe\n",
    "for i in title_tokens_tk:\n",
    "    listToStr = ' '.join(map(str, i))\n",
    "    title_list_temp.append(listToStr)\n",
    "    listToStr=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55449\n",
      "55449\n",
      "55449\n",
      "55449\n",
      "55449\n"
     ]
    }
   ],
   "source": [
    "#Validating the lengths of different attributes that are be saved in the job_ads.txt file.\n",
    "print(len(category))\n",
    "print(len(set(ids1)))\n",
    "print(len(webind))\n",
    "print(len(title1))\n",
    "print(len(desc_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Created a function to extract the values from attributes like webindex and category\n",
    "def FunSplit(temp_lis):\n",
    "    temp=[i.split(\": \")[1] for i in temp_lis]\n",
    "    return temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracted webindex values.\n",
    "webindex1=FunSplit(webind) #to be stored job ads dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracted category values.\n",
    "category1=FunSplit(category) #to be stored job ads dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracted title values.\n",
    "job_title1=title #to be stored job ads dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save output data as  job_ads.txt\n",
    "with open('job_ads.txt', 'w') as f:\n",
    "    for i,j,k,l,m in zip(ids1,category,webind,title1,desc_list):\n",
    "        f.write(\"%s\\n\" % i)\n",
    "        f.write(\"%s\\n\" % j)\n",
    "        f.write(\"%s\\n\" % k)\n",
    "        f.write(\"%s\\n\" % l)\n",
    "        f.write(\"%s\\n\" % m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created a dataframe structure with all relatable attributes of job advertisement data.\n",
    "job_df=pd.DataFrame()\n",
    "job_df[\"job title\"]=job_title1\n",
    "job_df[\"webindex\"]=webindex1\n",
    "job_df[\"category\"]=category1\n",
    "job_df[\"Job Description processed\"]=desc_list_temp\n",
    "job_df[\"Job title Description processed\"]=tle_desc_list_temp\n",
    "job_df[\"Job title processed\"]=title_list_temp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save dataframe data in job_ads1.csv, to read into task2_3\n",
    "#I created this csv to be read into task2. As based on thiswe can extract desired values from different attributes\n",
    "job_df.to_csv(\"job_ads1.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job Description token file to be used in task2\n",
    "\n",
    "with open('job_descrip1.txt', 'w') as f:\n",
    "    for i in desc_list_temp :\n",
    "        f.write(\"%s\\n\" % i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job title+Description token file to be used in task2\n",
    "\n",
    "with open('job_title_descrip1.txt', 'w') as f:\n",
    "    for i in tle_desc_list_temp :\n",
    "        f.write(\"%s\\n\" % i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "#job title token file to be used in task2\n",
    "\n",
    "with open('job_title1.txt', 'w') as f:\n",
    "    for i in title_list_temp :\n",
    "        f.write(\"%s\\n\" % i)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we put all the tokens in the corpus in a single list\n",
    "words_list_1 = list(chain.from_iterable(descrip_tokens_tk)) \n",
    "# computed the vocabulary by converting the list of words/tokens to a set, i.e., giving a set of unique words from job description.\n",
    "vocab_list = set(words_list_1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created a sorted function to extract the vocabs in a ascending order sorted manner\n",
    "def convert(set):\n",
    "    return sorted(set)\n",
    "\n",
    "vocab1=convert(vocab_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save output data in the required format- vocab.txt\n",
    "with open('vocab.txt', 'w') as f:\n",
    "   for i in range(0,len(vocab1)):\n",
    "        ss=vocab1[i]+\":\"+str(i)\n",
    "        f.write(\"%s\\n\" % ss)\n",
    "        ss=''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save output data... bigram.txt,to be saved in descending order.\n",
    "with open('bigram.txt', 'w') as f:\n",
    "    for i in range(0,len(bigram_desc)) :\n",
    "        c=str(bigram_desc[i][0]).split(\",\")\n",
    "        c1=c[0][2:-1]+\" \"+c[1][2:-2]+\",\"+str(bigram_desc[i][1])\n",
    "        f.write(\"%s\\n\" % c1)\n",
    "        c1=''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "Give a short summary and anything you would like to talk about the assessment task here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was a long assessment.So, i considered to save all the extracted tokens nd processed and unprocessed data into text and csv files respectively.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
