{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2: Milestone I Natural Language Processing\n",
    "## Task 2&3\n",
    "#### Student Name: Sai Vamsi Chunduru\n",
    "#### Student ID: s3884753\n",
    "\n",
    "Date: 02-10-2021\n",
    "\n",
    "Version: 1.0\n",
    "\n",
    "Environment: Python 3 and Jupyter notebook\n",
    "\n",
    "Libraries used: please include all the libraries you used in your assignment, e.g.,:\n",
    "* pandas\n",
    "* os\n",
    "* gensim.matutils.sparse2full\n",
    "* CountVectorizer\n",
    "* FastText\n",
    "* numpy\n",
    "* train_test_split\n",
    "* LogisticRegression\n",
    "* Dictionary\n",
    "* TfidfModel\n",
    "* gensim.downloader as api\n",
    "\n",
    "## Introduction\n",
    "Abrief information of this assessment task here.\n",
    "\n",
    "## Task 2\n",
    "I generated a different types of feature representations for the collection of job advertisements.In this task, we will only consider the description of the job advertisement.\n",
    "\n",
    "Bag-of-words model: Generated the Count vector representation for each job advertisement description.The generated Count vector\n",
    "representation is based on the generated vocabulary in Task 1 (as saved in vocab.txt).\n",
    "\n",
    "Models based on word embeddings:\n",
    "we generate feature representation of job advertisement description based on\n",
    "the following language models, respectively:\n",
    "■ FastText language model trained based on the provided job advertisement\n",
    "descriptions, with a 200 embedding dimension.\n",
    "■ 2 pre-trained language models- GoogleNews300 and Glove.\n",
    "\n",
    "For each of the above mentioned language models, I built the weighted (i.e., TF-IDF weighted) and unweighted vector representation for each job advertisement\n",
    "\n",
    "count_vectors.txt- This file stores the sparse count vector representation of job advertisement descriptions in the required format. \n",
    "\n",
    "## Task 3\n",
    "\n",
    "In this task, we build machine learning models for classifying the category of a job advertisement text.\n",
    "\n",
    "A simple model that can be considered is the logistic regression model from sklearn.\n",
    "\n",
    "I conducted two sets of experiments on the provided dataset to investigate the following two questions, respectively.\n",
    "\n",
    "Q1: Language model comparison : Which language model we built based on job advertisement descriptions performs the best with\n",
    "the chosen machine learning model.\n",
    "\n",
    "To answer these questions, we build machine learning models based on the feature representations of the documents.We created 7 different models- 1 model based on count vector representations, 2 models of FastText(unweighted and weighted tfidf), 2 models of GoogleNews300(unweighted and weighted tfidf), and 2 models of Glove(unweighted and weighted tfidf). \n",
    "We evaluated the various model performance based on their model scores.\n",
    "\n",
    "Q2: Does more information provide higher accuracy?Will adding extra information help to boost up the accuracy of the model? To answer this question,\n",
    "\n",
    "In this experiment we explore other features of a job advertisement, e.g., the title of the job\n",
    "position. and we looked to answer this question.\n",
    "So to answer the above question I considered 2 additional data sets like job title + escription and job title data alone from job advertisement datasets. I built a FastText unweighted model based and compared the performance of classification models with the FastText model built on job description data alone.\n",
    "I Considered unweighted FastText model to comare these 3 different (job description, job title+ description and job title) classification models. \n",
    "\n",
    "  Reasons for considering Unweighted FastText model: This model displayed much higher accuracy score along with weighted fastText model. \n",
    "This is expected cause we bulit the FastText model based  on our processed data rather than on the pre trained models.\n",
    "Eventhough weighted FastText model accuracy is higher than unweighted Fasttext model.We used unweighted FastText model due it's minimal computational resource usage.\n",
    "I used different solvers (sag and saga) of logistic regression to compare the model performances, but the default solver \"ibfgs\" performs better than others. So we used ibfgs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing libraries "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to import libraries that we need in this assessment.\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from gensim.matutils import sparse2full\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from gensim.models.fasttext import FastText\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.models.tfidfmodel import TfidfModel\n",
    "import gensim.downloader as api\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2. Generating Feature Representations for Job Advertisement Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...... Sections and code blocks on buidling different document feature represetations\n",
    "\n",
    "\n",
    "<span style=\"color: red\"> You might have complex notebook structure in this section, please feel free to create your own notebook structure. </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#readinf the job_ads1 csv file this is saved in task-1, used here extract the data ina data frame structure\n",
    "job_ads_df=pd.read_csv(\"job_ads1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>job title</th>\n",
       "      <th>webindex</th>\n",
       "      <th>category</th>\n",
       "      <th>Job Description processed</th>\n",
       "      <th>Job title Description processed</th>\n",
       "      <th>Job title processed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Plant Engineer</td>\n",
       "      <td>62119057</td>\n",
       "      <td>Engineering</td>\n",
       "      <td>established manufacturer supplier quality wate...</td>\n",
       "      <td>plant engineer established manufacturer suppli...</td>\n",
       "      <td>plant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Residential Care Worker</td>\n",
       "      <td>66314490</td>\n",
       "      <td>Healthcare_Nursing</td>\n",
       "      <td>timeout children homes rapidly expanding foref...</td>\n",
       "      <td>residential care worker timeout children homes...</td>\n",
       "      <td>residential</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CHEF DE RANG FOR MICHELIN STARRED RESTAURANT</td>\n",
       "      <td>69182387</td>\n",
       "      <td>Hospitality_Catering</td>\n",
       "      <td>french restaurant club gascon michelin establi...</td>\n",
       "      <td>chef de rang michelin starred restaurant frenc...</td>\n",
       "      <td>rang michelin starred</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Inpatient Ward Team Leader</td>\n",
       "      <td>64752715</td>\n",
       "      <td>Healthcare_Nursing</td>\n",
       "      <td>inpatient ward leader description patient lead...</td>\n",
       "      <td>inpatient ward leader inpatient ward leader de...</td>\n",
       "      <td>inpatient ward leader</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      job title  webindex  \\\n",
       "0                                Plant Engineer  62119057   \n",
       "1                       Residential Care Worker  66314490   \n",
       "2  CHEF DE RANG FOR MICHELIN STARRED RESTAURANT  69182387   \n",
       "3                    Inpatient Ward Team Leader  64752715   \n",
       "\n",
       "               category                          Job Description processed  \\\n",
       "0           Engineering  established manufacturer supplier quality wate...   \n",
       "1    Healthcare_Nursing  timeout children homes rapidly expanding foref...   \n",
       "2  Hospitality_Catering  french restaurant club gascon michelin establi...   \n",
       "3    Healthcare_Nursing  inpatient ward leader description patient lead...   \n",
       "\n",
       "                     Job title Description processed    Job title processed  \n",
       "0  plant engineer established manufacturer suppli...                  plant  \n",
       "1  residential care worker timeout children homes...            residential  \n",
       "2  chef de rang michelin starred restaurant frenc...  rang michelin starred  \n",
       "3  inpatient ward leader inpatient ward leader de...  inpatient ward leader  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the 4 columns of dataframe.\n",
    "job_ads_df.head(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55449"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validating the length of categories.\n",
    "len(job_ads_df[\"category\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Storing the categories in a list structure.\n",
    "category12=job_ads_df[\"category\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking for null values in category\n",
    "job_ads_df[\"category\"].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the job title-unprocessed in list format.\n",
    "job_title=job_ads_df[\"job title\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the job description-processed in list format\n",
    "jobads12=job_ads_df['Job Description processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the job description into token and saving them in a list format\n",
    "job_desc_tokens=[i.split(\" \") for i in jobads12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the job title + description-processed in list format\n",
    "jobads13=job_ads_df['Job title Description processed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting the job title+ description into token and saving them in a list format\n",
    "job_title_desc_tokens=[i.split(\" \") for i in jobads13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the job title -processed in list format\n",
    "job_title_tokens=job_ads_df['Job title processed'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "str"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Validating the data type of elements\n",
    "type(job_title_tokens[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#storing the job webindex in list format\n",
    "webindex12=job_ads_df['webindex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading the vocabulary from vocb.txt-from task 1\n",
    "voc1=[]\n",
    "with open('vocab.txt') as f:\n",
    "    for line in f:\n",
    "        voc1.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55449\n",
      "40038\n",
      "55449\n"
     ]
    }
   ],
   "source": [
    "#Displaying the sizes of job advertisements data, vocbulary and webindex.\n",
    "print(len(jobads12))\n",
    "print(len(voc1))\n",
    "print(len(webindex12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "established manufacturer supplier quality water treatment plants ranging basic water softeners reverse osmosis equipment customer complex water treatment solutions meet clients requirements flexibility tailoring product budgets due expansion increased workload seeking recruit planet engineer cover accounts corridor responsibilities include conducting routine sampling analysis water systems interpreting results maintenance installation chemical dosing systems servicing accounts industrial commercial industries complete accordance approved code practice ideal applicant minimum years relevant industry reverse osmosis water softeners water filters uv equipment full driving license return offering competitive package ideal\n",
      "a'level:0\n",
      "\n",
      "62119057\n"
     ]
    }
   ],
   "source": [
    "#Displaying the sample data present in jobads-description,vocab and webindex\n",
    "print(jobads12[0])\n",
    "print(voc1[0])\n",
    "print(webindex12[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#converting webindex data type to string\n",
    "webindex=[str(i) for i in webindex12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#splitting and saving the extracted vocab in a list format.\n",
    "vocab1=[i.split(\":\")[0] for i in voc1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"a'level\", 'aa', 'aaa', 'aaappointments', 'aab']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying sample vocabs\n",
    "vocab1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code to perform the task...\n",
    "#Creating count vector representation\n",
    "cVectorizer = CountVectorizer(analyzer = \"word\",vocabulary = vocab1) # initialised the CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55449, 40038)\n"
     ]
    }
   ],
   "source": [
    "count_features = cVectorizer.fit_transform([' '.join(desc) for desc in job_desc_tokens])\n",
    "# generate the count vector representation for all words in job description\n",
    "print(count_features.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#created a count_vec_list1 to hold webindex and count features data.\n",
    "count_vec_list1=[]\n",
    "strz12=''\n",
    "for i in count_features:\n",
    "    t1=i.toarray()\n",
    "    t2=t1[0]\n",
    "    for word_index, value in zip(range(0, len(vocab1)), t2): # using word_index:value format\n",
    "        if value > 0:\n",
    "           strz12=strz12+str(word_index)+\":\"+str(value)+\" \"  \n",
    "    count_vec_list1.append(strz12)\n",
    "    strz12=''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "55449"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the length of extracted count_vec_list1 list.\n",
    "len(count_vec_list1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Replacing the spaces in the spaces in count_vec_list1 with commas(,) as per requirements.\n",
    "count_vec=[i.replace(\" \", \",\")[0:-1] for i in count_vec_list1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_docVecs_modified1(wv,tk_txts): # generate vector representation for documents\n",
    "    docs_vectors =[] # creating empty final list\n",
    "    for i in range(0,len(tk_txts)):\n",
    "            tokens = tk_txts[i]\n",
    "            temp = []  # creating a temporary list(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
    "            temp2=[]   #It holds the temp values after each iteration\n",
    "            for w_ind in range(0, len(tokens)): # looping through each word of a single document and spliting through space\n",
    "                try:\n",
    "                    word = tokens[w_ind]\n",
    "                    word_vec = wv[word] # if word is present in embeddings(google provides weights associate with words(300)/fasttext with size 200) then proceed\n",
    "                    temp = pd.Series(word_vec) # if word is present then assign it to temp.\n",
    "                    temp2.append(temp)\n",
    "                except:\n",
    "                    pass\n",
    "                temp=[]\n",
    "            zk=np.sum(np.array(temp2),axis=0) #The sum is calculated and stored in zk, a temp variable\n",
    "            docs_vectors.append(zk) #toring all the doc vectors in this list.\n",
    "    return docs_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=22816, vector_size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Code to perform the task...\n",
    "#Generating document vectors with FastText model.\n",
    "# 1. Set the corpus file names/path\n",
    "corpus_file = './job_descrip1.txt'\n",
    "\n",
    "# 2. Initialise the Fast Text model\n",
    "jobDescFT = FastText(vector_size=200) \n",
    "\n",
    "# 3. build the vocabulary\n",
    "jobDescFT.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "# 4. train the model\n",
    "jobDescFT.train(\n",
    "    corpus_file=corpus_file, epochs=jobDescFT.epochs,\n",
    "    total_examples=jobDescFT.corpus_count, total_words=jobDescFT.corpus_total_words,\n",
    ")\n",
    "\n",
    "print(jobDescFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastTextKeyedVectors object at 0x000001AC80D45100>\n"
     ]
    }
   ],
   "source": [
    "#Displaying the job description word vectors.\n",
    "jobDescFT_wv = jobDescFT.wv\n",
    "print(jobDescFT_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the FastText model-can be used directly wthout building amodel.\n",
    "jobDescFT.save(\"jobDescFT.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=22816, vector_size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# loading the trained Fasttext model based on bbc News data\n",
    "#We can load this model or can comment this whole block,\n",
    "jobDescFT = FastText.load(\"jobDescFT.model\")\n",
    "print(jobDescFT)\n",
    "jobDescFT_wv= jobDescFT.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55449, 200)\n"
     ]
    }
   ],
   "source": [
    "# NOTE this takes some time to finish running\n",
    "# generate document embeddings.\n",
    "jobDescFT_dvs12 = gen_docVecs_modified1(jobDescFT_wv,job_desc_tokens)\n",
    "jobDescFT_dvs_arr=np.array(jobDescFT_dvs12) #Converting the generated document vectors into numpy arrays.\n",
    "print(jobDescFT_dvs_arr.shape)  # displaying the shape of array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imported gensim loader to load googlenews 300 pre trained model.\n",
    "import gensim.downloader as api\n",
    "preTW2v_g300_wv = api.load('word2vec-google-news-300')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#checking the vector size of extracted google 300-news, word vector\n",
    "preTW2v_g300_wv.vector_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55449, 300)\n"
     ]
    }
   ],
   "source": [
    "# NOTE this can take some time to finish running\n",
    "# generate document embeddings\n",
    "jobDesc_g300_dvs12 = gen_docVecs_modified1(preTW2v_g300_wv,job_desc_tokens)\n",
    "jobDesc_g300_dvs_arr=np.array(jobDesc_g300_dvs12)  #Converting the generated document vectors into numpy arrays.\n",
    "print(jobDesc_g300_dvs_arr.shape) # displaying the shape of array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generating word vectors based on pre trained model-golve\n",
    "path_to_glove_file = os.path.join(\"glove/glove.6B.200d.txt\") # specified the path to the embedding file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "glove_wv = {} # initialise an empty dcitionary\n",
    "with open(path_to_glove_file, encoding=\"utf-8\") as f: # open the txt file containing the word embedding vectors\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1) # The maxsplit defines the maximum number of splits. \n",
    "                                             # in the above example, it will give:\n",
    "                                             # ['population','0.035182 1.4248 0.9758 0.1313 -0.66877 0.8539 -0.11525 ......']\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \") # construct an numpy array from the string 'coefs', \n",
    "                                                   # e.g., '0.035182 1.4248 0.9758 0.1313 -0.66877 0.8539 -0.11525 ......'\n",
    "        glove_wv[word] = coefs # create the word and embedding vector mapping\n",
    "\n",
    "print(\"Found %s word vectors.\" % len(glove_wv))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE this can take some time to finish running\n",
    "# generate document embeddings-unweighted\n",
    "jobDesc_glove_dvs12 = gen_docVecs_modified1(glove_wv,job_desc_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55449, 200)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jobDesc_glove_dvs_arr=np.array(jobDesc_glove_dvs12) #Converting the generated document vectors into numpy arrays.\n",
    "jobDesc_glove_dvs_arr.shape # displaying the shape of array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55449, 40038)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculating the tfidf vecors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tVectorizer = TfidfVectorizer(analyzer = \"word\",vocabulary = vocab1) # initialised the TfidfVectorizer\n",
    "tfidf_features = tVectorizer.fit_transform([' '.join(desc) for desc in job_desc_tokens]) # generate the tfidf vector representation for all word in job description\n",
    "tfidf_features.shape #displaying the shape of extracted tfidf_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#formatting the vocabulary in the required dictonary form.\n",
    "def gen_vocIndex(voc1):\n",
    "    voc_Ind = [l.split(':') for l in voc1] # each line is 'index,word'\n",
    "    return {int(vi[1]):vi[0] for vi in voc_Ind}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: \"a'level\",\n",
       " 1: 'aa',\n",
       " 2: 'aaa',\n",
       " 3: 'aaappointments',\n",
       " 4: 'aab',\n",
       " 5: 'aac',\n",
       " 6: 'aacc',\n",
       " 7: 'aae',\n",
       " 8: 'aah',\n",
       " 9: 'aak',\n",
       " 10: 'aamm',\n",
       " 11: 'aan',\n",
       " 12: 'aand',\n",
       " 13: 'aantrekkelijk',\n",
       " 14: 'aap',\n",
       " 15: 'aar',\n",
       " 16: 'aarca',\n",
       " 17: 'aardman',\n",
       " 18: 'aareon',\n",
       " 19: 'aaron',\n",
       " 20: 'aaronwallis',\n",
       " 21: 'aarosette',\n",
       " 22: 'aarosettehotel',\n",
       " 23: 'aarosettelivein',\n",
       " 24: 'aarosetterestaurant',\n",
       " 25: 'aarosetterestaurantlondon',\n",
       " 26: 'aarosettes',\n",
       " 27: 'aarosettesinternationalchainofhotels',\n",
       " 28: 'aarosetteslivein',\n",
       " 29: 'aasl',\n",
       " 30: 'aasp',\n",
       " 31: 'aastra',\n",
       " 32: 'aat',\n",
       " 33: 'aatom',\n",
       " 34: 'ab',\n",
       " 35: 'aba',\n",
       " 36: 'abacus',\n",
       " 37: 'abaility',\n",
       " 38: 'abandoned',\n",
       " 39: 'abandonment',\n",
       " 40: 'abandons',\n",
       " 41: 'abap',\n",
       " 42: 'abaqus',\n",
       " 43: 'abatement',\n",
       " 44: 'abb',\n",
       " 45: 'abbas',\n",
       " 46: 'abbey',\n",
       " 47: 'abbeywood',\n",
       " 48: 'abbie',\n",
       " 49: 'abbot',\n",
       " 50: 'abbots',\n",
       " 51: 'abbott',\n",
       " 52: 'abby',\n",
       " 53: 'abbyy',\n",
       " 54: 'abc',\n",
       " 55: 'abd',\n",
       " 56: 'abdo',\n",
       " 57: 'abdominal',\n",
       " 58: 'abdul',\n",
       " 59: 'abdulla',\n",
       " 60: 'abel',\n",
       " 61: 'abellprotocoleducation',\n",
       " 62: 'abenefit',\n",
       " 63: 'aberdare',\n",
       " 64: 'aberdeen',\n",
       " 65: \"aberdeen's\",\n",
       " 66: 'aberdeenshire',\n",
       " 67: 'aberdenshire',\n",
       " 68: 'aberfeldy',\n",
       " 69: 'abergavenny',\n",
       " 70: 'abertillery',\n",
       " 71: 'aberystwyth',\n",
       " 72: 'abfa',\n",
       " 73: 'abgeschlossenes',\n",
       " 74: 'abi',\n",
       " 75: 'abid',\n",
       " 76: 'abide',\n",
       " 77: 'abiding',\n",
       " 78: 'abigail',\n",
       " 79: 'abili',\n",
       " 80: 'abilities',\n",
       " 81: 'abilty',\n",
       " 82: 'abingdon',\n",
       " 83: 'abinitio',\n",
       " 84: 'abl',\n",
       " 85: 'ablity',\n",
       " 86: 'ably',\n",
       " 87: 'abm',\n",
       " 88: 'abnormal',\n",
       " 89: 'abnormalities',\n",
       " 90: 'abnormality',\n",
       " 91: 'aboard',\n",
       " 92: 'abobe',\n",
       " 93: 'abode',\n",
       " 94: 'abortion',\n",
       " 95: 'abortive',\n",
       " 96: 'abound',\n",
       " 97: 'aboutus',\n",
       " 98: 'abovetheline',\n",
       " 99: 'aboyne',\n",
       " 100: 'abp',\n",
       " 101: 'abpi',\n",
       " 102: 'abpm',\n",
       " 103: 'abr',\n",
       " 104: 'abraham',\n",
       " 105: 'abrahamrandstad',\n",
       " 106: 'abram',\n",
       " 107: 'abrasive',\n",
       " 108: 'abrasives',\n",
       " 109: 'abrassociates',\n",
       " 110: 'abraxa',\n",
       " 111: 'abreast',\n",
       " 112: 'abrecco',\n",
       " 113: 'abrecruit',\n",
       " 114: 'abreu',\n",
       " 115: 'abroad',\n",
       " 116: 'abrs',\n",
       " 117: 'abs',\n",
       " 118: 'abscence',\n",
       " 119: 'absconding',\n",
       " 120: 'absence',\n",
       " 121: 'absences',\n",
       " 122: 'absent',\n",
       " 123: 'absenteeism',\n",
       " 124: 'absolute',\n",
       " 125: 'absolutely',\n",
       " 126: 'absorb',\n",
       " 127: 'absorbed',\n",
       " 128: 'absorbing',\n",
       " 129: 'absorption',\n",
       " 130: 'abstract',\n",
       " 131: 'abstracted',\n",
       " 132: 'abstraction',\n",
       " 133: 'abstracts',\n",
       " 134: 'abu',\n",
       " 135: 'abundance',\n",
       " 136: 'abundant',\n",
       " 137: 'aburnspremiermedia',\n",
       " 138: 'abuse',\n",
       " 139: 'abused',\n",
       " 140: 'abut',\n",
       " 141: 'ac',\n",
       " 142: 'aca',\n",
       " 143: 'acab',\n",
       " 144: 'acac',\n",
       " 145: 'acad',\n",
       " 146: 'academia',\n",
       " 147: 'academic',\n",
       " 148: 'academically',\n",
       " 149: 'academics',\n",
       " 150: 'academies',\n",
       " 151: 'academy',\n",
       " 152: \"academy's\",\n",
       " 153: 'academys',\n",
       " 154: 'acams',\n",
       " 155: 'acando',\n",
       " 156: 'acarrmedicorglobal',\n",
       " 157: 'acarrstrgroup',\n",
       " 158: 'acas',\n",
       " 159: 'acc',\n",
       " 160: 'acca',\n",
       " 161: 'accelerate',\n",
       " 162: 'accelerated',\n",
       " 163: 'accelerating',\n",
       " 164: 'acceleration',\n",
       " 165: 'accelerator',\n",
       " 166: 'accelerators',\n",
       " 167: 'accelerit',\n",
       " 168: 'accelerometer',\n",
       " 169: 'accent',\n",
       " 170: 'accenture',\n",
       " 171: 'accept',\n",
       " 172: 'acceptability',\n",
       " 173: 'acceptable',\n",
       " 174: 'acceptance',\n",
       " 175: 'acceptances',\n",
       " 176: 'accepted',\n",
       " 177: 'accepting',\n",
       " 178: 'accepts',\n",
       " 179: 'acces',\n",
       " 180: 'accesible',\n",
       " 181: 'access',\n",
       " 182: 'accessed',\n",
       " 183: 'accesses',\n",
       " 184: 'accessibilities',\n",
       " 185: 'accessibility',\n",
       " 186: 'accessible',\n",
       " 187: 'accessing',\n",
       " 188: 'accessor',\n",
       " 189: 'accessories',\n",
       " 190: 'accessory',\n",
       " 191: 'accident',\n",
       " 192: 'accidental',\n",
       " 193: 'accidents',\n",
       " 194: 'acclaim',\n",
       " 195: 'acclaimed',\n",
       " 196: 'acclimatise',\n",
       " 197: 'accolade',\n",
       " 198: 'accoladed',\n",
       " 199: 'accolades',\n",
       " 200: 'accom',\n",
       " 201: 'accomadation',\n",
       " 202: 'accomadtion',\n",
       " 203: 'accommodate',\n",
       " 204: 'accommodated',\n",
       " 205: 'accommodates',\n",
       " 206: 'accommodating',\n",
       " 207: 'accommodation',\n",
       " 208: 'accommodations',\n",
       " 209: 'accomodate',\n",
       " 210: 'accomodation',\n",
       " 211: 'accompanied',\n",
       " 212: 'accompanies',\n",
       " 213: 'accompaniments',\n",
       " 214: 'accompany',\n",
       " 215: 'accompanying',\n",
       " 216: 'accomplish',\n",
       " 217: 'accomplished',\n",
       " 218: 'accomplishes',\n",
       " 219: 'accomplishing',\n",
       " 220: 'accomplishment',\n",
       " 221: 'accomplishments',\n",
       " 222: 'accomtype',\n",
       " 223: 'accor',\n",
       " 224: 'accord',\n",
       " 225: 'accordance',\n",
       " 226: 'accords',\n",
       " 227: 'accorecruitment',\n",
       " 228: 'account',\n",
       " 229: \"account's\",\n",
       " 230: 'accountabilites',\n",
       " 231: 'accountabilities',\n",
       " 232: 'accountability',\n",
       " 233: \"accountability's\",\n",
       " 234: 'accountable',\n",
       " 235: 'accountancy',\n",
       " 236: 'accountancyworldwide',\n",
       " 237: 'accountant',\n",
       " 238: \"accountant's\",\n",
       " 239: 'accountantbelfast',\n",
       " 240: 'accountants',\n",
       " 241: 'accounted',\n",
       " 242: 'accountex',\n",
       " 243: 'accounting',\n",
       " 244: 'accountmanagerb',\n",
       " 245: 'accountmanagerdigitalprintproduction',\n",
       " 246: 'accounts',\n",
       " 247: 'accpac',\n",
       " 248: 'accra',\n",
       " 249: 'accredit',\n",
       " 250: 'accreditation',\n",
       " 251: \"accreditation's\",\n",
       " 252: 'accreditations',\n",
       " 253: 'accredited',\n",
       " 254: 'accreditors',\n",
       " 255: 'accreditted',\n",
       " 256: 'accrington',\n",
       " 257: 'accros',\n",
       " 258: 'accross',\n",
       " 259: 'accrual',\n",
       " 260: 'accruals',\n",
       " 261: 'accrue',\n",
       " 262: 'accrued',\n",
       " 263: 'accruement',\n",
       " 264: 'accruing',\n",
       " 265: 'accsim',\n",
       " 266: 'acct',\n",
       " 267: 'accumen',\n",
       " 268: 'accumulate',\n",
       " 269: 'accumulated',\n",
       " 270: 'accumulating',\n",
       " 271: 'accumulation',\n",
       " 272: 'accuracy',\n",
       " 273: 'accurate',\n",
       " 274: 'accurately',\n",
       " 275: 'accuratelyto',\n",
       " 276: 'accurev',\n",
       " 277: 'accustomed',\n",
       " 278: 'accute',\n",
       " 279: 'acd',\n",
       " 280: 'acdc',\n",
       " 281: 'ace',\n",
       " 282: 'acer',\n",
       " 283: 'acf',\n",
       " 284: 'ach',\n",
       " 285: 'acheiev',\n",
       " 286: 'acheive',\n",
       " 287: 'acheiver',\n",
       " 288: 'achievable',\n",
       " 289: 'achieve',\n",
       " 290: 'achieved',\n",
       " 291: 'achieveing',\n",
       " 292: 'achievement',\n",
       " 293: 'achievements',\n",
       " 294: 'achiever',\n",
       " 295: 'achievers',\n",
       " 296: 'achieves',\n",
       " 297: 'achieving',\n",
       " 298: 'achive',\n",
       " 299: 'achtergrond',\n",
       " 300: 'acia',\n",
       " 301: 'acib',\n",
       " 302: 'acid',\n",
       " 303: 'acii',\n",
       " 304: 'acila',\n",
       " 305: 'acis',\n",
       " 306: 'acknowledge',\n",
       " 307: 'acknowledged',\n",
       " 308: 'acknowledgement',\n",
       " 309: 'acknowledgements',\n",
       " 310: 'acknowledges',\n",
       " 311: 'acknowledging',\n",
       " 312: 'acknowledgments',\n",
       " 313: 'ackworth',\n",
       " 314: 'acl',\n",
       " 315: 'acls',\n",
       " 316: 'acm',\n",
       " 317: 'acma',\n",
       " 318: 'acme',\n",
       " 319: 'acmp',\n",
       " 320: 'acne',\n",
       " 321: 'acocks',\n",
       " 322: 'acoenfrlondon',\n",
       " 323: 'acomb',\n",
       " 324: 'acompetitive',\n",
       " 325: 'acop',\n",
       " 326: \"acop's\",\n",
       " 327: 'acops',\n",
       " 328: 'acord',\n",
       " 329: 'acorn',\n",
       " 330: 'acorss',\n",
       " 331: 'acounts',\n",
       " 332: 'acoustic',\n",
       " 333: 'acoustical',\n",
       " 334: 'acoustics',\n",
       " 335: 'acoustooptic',\n",
       " 336: 'acp',\n",
       " 337: 'acpo',\n",
       " 338: 'acquaintance',\n",
       " 339: 'acquainted',\n",
       " 340: 'acquire',\n",
       " 341: 'acquired',\n",
       " 342: 'acquirer',\n",
       " 343: 'acquirers',\n",
       " 344: 'acquiring',\n",
       " 345: 'acquisition',\n",
       " 346: 'acquisitional',\n",
       " 347: 'acquisitions',\n",
       " 348: 'acquisitive',\n",
       " 349: 'acr',\n",
       " 350: 'acre',\n",
       " 351: 'acreddited',\n",
       " 352: 'acres',\n",
       " 353: 'acrib',\n",
       " 354: 'acrobat',\n",
       " 355: 'acromas',\n",
       " 356: 'acronis',\n",
       " 357: 'acrp',\n",
       " 358: 'acs',\n",
       " 359: 'acsa',\n",
       " 360: 'acsp',\n",
       " 361: 'acss',\n",
       " 362: 'act',\n",
       " 363: 'actdesirable',\n",
       " 364: 'acted',\n",
       " 365: 'actel',\n",
       " 366: 'actelis',\n",
       " 367: 'actief',\n",
       " 368: 'acting',\n",
       " 369: 'action',\n",
       " 370: 'actionable',\n",
       " 371: 'actioned',\n",
       " 372: 'actioning',\n",
       " 373: 'actionoriented',\n",
       " 374: 'actions',\n",
       " 375: 'actionscript',\n",
       " 376: 'actionscripting',\n",
       " 377: 'actionvacancymgt',\n",
       " 378: 'activate',\n",
       " 379: 'activated',\n",
       " 380: 'activates',\n",
       " 381: 'activating',\n",
       " 382: 'activation',\n",
       " 383: 'activations',\n",
       " 384: 'active',\n",
       " 385: 'activeh',\n",
       " 386: 'activeidentity',\n",
       " 387: 'activeled',\n",
       " 388: 'actively',\n",
       " 389: 'activematrix',\n",
       " 390: 'activemq',\n",
       " 391: 'actives',\n",
       " 392: 'activesync',\n",
       " 393: 'activex',\n",
       " 394: 'activists',\n",
       " 395: 'activites',\n",
       " 396: 'activiti',\n",
       " 397: 'activities',\n",
       " 398: 'activitiesfor',\n",
       " 399: 'activitly',\n",
       " 400: 'activity',\n",
       " 401: 'activley',\n",
       " 402: 'acton',\n",
       " 403: 'actonhighschool',\n",
       " 404: 'actors',\n",
       " 405: 'acts',\n",
       " 406: 'actual',\n",
       " 407: 'actuals',\n",
       " 408: 'actuariaat',\n",
       " 409: 'actuarial',\n",
       " 410: 'actuaries',\n",
       " 411: 'actuary',\n",
       " 412: 'actuate',\n",
       " 413: 'actuation',\n",
       " 414: 'actuator',\n",
       " 415: 'actuators',\n",
       " 416: 'acturis',\n",
       " 417: 'acuity',\n",
       " 418: 'acumen',\n",
       " 419: 'acumenexcellent',\n",
       " 420: 'acupuncture',\n",
       " 421: 'acurately',\n",
       " 422: 'acute',\n",
       " 423: 'acutely',\n",
       " 424: 'acv',\n",
       " 425: 'acwp',\n",
       " 426: 'ad',\n",
       " 427: 'ada',\n",
       " 428: 'adabas',\n",
       " 429: 'adam',\n",
       " 430: 'adambeerecruitment',\n",
       " 431: 'adambutlerltd',\n",
       " 432: 'adammortimerbell',\n",
       " 433: 'adamprecedohealthcare',\n",
       " 434: 'adamredoakrecruitment',\n",
       " 435: 'adams',\n",
       " 436: 'adamsargeantpartnership',\n",
       " 437: 'adamson',\n",
       " 438: 'adapt',\n",
       " 439: 'adaptability',\n",
       " 440: 'adaptable',\n",
       " 441: 'adaptablity',\n",
       " 442: 'adaptation',\n",
       " 443: 'adaptations',\n",
       " 444: 'adapted',\n",
       " 445: 'adapters',\n",
       " 446: 'adapting',\n",
       " 447: 'adaption',\n",
       " 448: 'adaptions',\n",
       " 449: 'adaptive',\n",
       " 450: 'adaptne',\n",
       " 451: 'adaptors',\n",
       " 452: 'adapts',\n",
       " 453: 'adare',\n",
       " 454: 'adas',\n",
       " 455: 'adbrite',\n",
       " 456: 'adc',\n",
       " 457: 'adcenter',\n",
       " 458: 'adcs',\n",
       " 459: 'add',\n",
       " 460: 'added',\n",
       " 461: 'addedvalue',\n",
       " 462: 'addenbrookes',\n",
       " 463: 'addendums',\n",
       " 464: 'adderley',\n",
       " 465: 'addiction',\n",
       " 466: 'addictions',\n",
       " 467: 'addictive',\n",
       " 468: 'addie',\n",
       " 469: 'adding',\n",
       " 470: 'addington',\n",
       " 471: 'addins',\n",
       " 472: 'addition',\n",
       " 473: 'additional',\n",
       " 474: 'additionally',\n",
       " 475: 'additions',\n",
       " 476: 'additives',\n",
       " 477: 'additon',\n",
       " 478: 'additonal',\n",
       " 479: 'addlestone',\n",
       " 480: 'addon',\n",
       " 481: 'addons',\n",
       " 482: 'address',\n",
       " 483: 'addressable',\n",
       " 484: 'addressed',\n",
       " 485: 'addresses',\n",
       " 486: 'addressing',\n",
       " 487: 'adds',\n",
       " 488: 'addtion',\n",
       " 489: 'addtional',\n",
       " 490: 'addtocart',\n",
       " 491: 'addysg',\n",
       " 492: 'addysgu',\n",
       " 493: 'ade',\n",
       " 494: 'adecco',\n",
       " 495: 'adeiladu',\n",
       " 496: 'adel',\n",
       " 497: 'adelaide',\n",
       " 498: 'adele',\n",
       " 499: 'adeline',\n",
       " 500: 'adelman',\n",
       " 501: 'adept',\n",
       " 502: 'adequacy',\n",
       " 503: 'adequate',\n",
       " 504: 'adequately',\n",
       " 505: 'aderant',\n",
       " 506: 'adestra',\n",
       " 507: 'adf',\n",
       " 508: 'adfs',\n",
       " 509: 'adhd',\n",
       " 510: 'adherance',\n",
       " 511: 'adhere',\n",
       " 512: 'adhered',\n",
       " 513: 'adherence',\n",
       " 514: 'adheres',\n",
       " 515: 'adhering',\n",
       " 516: 'adhesive',\n",
       " 517: 'adhesives',\n",
       " 518: 'adhoc',\n",
       " 519: 'adi',\n",
       " 520: 'adic',\n",
       " 521: 'adidas',\n",
       " 522: 'adis',\n",
       " 523: 'adjacent',\n",
       " 524: 'adjoining',\n",
       " 525: 'adjudicate',\n",
       " 526: 'adjudication',\n",
       " 527: 'adjudicator',\n",
       " 528: 'adjudicators',\n",
       " 529: 'adjust',\n",
       " 530: 'adjustable',\n",
       " 531: 'adjusted',\n",
       " 532: 'adjuster',\n",
       " 533: 'adjusters',\n",
       " 534: 'adjusting',\n",
       " 535: 'adjustment',\n",
       " 536: 'adjustments',\n",
       " 537: 'adjustors',\n",
       " 538: 'adjusts',\n",
       " 539: 'adl',\n",
       " 540: 'adlib',\n",
       " 541: 'adm',\n",
       " 542: 'adma',\n",
       " 543: 'admin',\n",
       " 544: 'admincharterhouserecruitment',\n",
       " 545: 'admindatasourcerecruitment',\n",
       " 546: 'admineclypserecruitment',\n",
       " 547: 'admininister',\n",
       " 548: 'administer',\n",
       " 549: 'administered',\n",
       " 550: 'administering',\n",
       " 551: 'administers',\n",
       " 552: 'administrate',\n",
       " 553: 'administrating',\n",
       " 554: 'administration',\n",
       " 555: 'administrational',\n",
       " 556: 'administrations',\n",
       " 557: 'administrative',\n",
       " 558: 'administratively',\n",
       " 559: 'administrator',\n",
       " 560: 'administratornorth',\n",
       " 561: 'administrators',\n",
       " 562: 'adminitrator',\n",
       " 563: 'adminluton',\n",
       " 564: 'adminp',\n",
       " 565: 'admins',\n",
       " 566: 'adminsteppingout',\n",
       " 567: 'adminstration',\n",
       " 568: 'adminstrator',\n",
       " 569: 'adminstudio',\n",
       " 570: 'adminview',\n",
       " 571: 'admirable',\n",
       " 572: 'admiral',\n",
       " 573: 'admiralgrp',\n",
       " 574: 'admiration',\n",
       " 575: 'admire',\n",
       " 576: 'admired',\n",
       " 577: 'admission',\n",
       " 578: 'admissions',\n",
       " 579: 'admit',\n",
       " 580: 'admits',\n",
       " 581: 'admitted',\n",
       " 582: 'admittedly',\n",
       " 583: 'admixtures',\n",
       " 584: 'admob',\n",
       " 585: 'adms',\n",
       " 586: 'adn',\n",
       " 587: 'adnan',\n",
       " 588: 'adnetworks',\n",
       " 589: 'adnoddau',\n",
       " 590: 'ado',\n",
       " 591: 'adobe',\n",
       " 592: 'adolescence',\n",
       " 593: 'adolescent',\n",
       " 594: \"adolescent's\",\n",
       " 595: 'adolescents',\n",
       " 596: 'adops',\n",
       " 597: 'adopt',\n",
       " 598: 'adoptable',\n",
       " 599: 'adopted',\n",
       " 600: 'adopter',\n",
       " 601: 'adopters',\n",
       " 602: 'adopting',\n",
       " 603: 'adoption',\n",
       " 604: 'adopts',\n",
       " 605: 'adorned',\n",
       " 606: 'adp',\n",
       " 607: 'adplanner',\n",
       " 608: 'adpro',\n",
       " 609: 'adr',\n",
       " 610: 'adran',\n",
       " 611: 'adrelevance',\n",
       " 612: 'adrem',\n",
       " 613: 'adrenaline',\n",
       " 614: 'adria',\n",
       " 615: 'adrian',\n",
       " 616: 'adriancavendishmaine',\n",
       " 617: 'adrienne',\n",
       " 618: 'adrodd',\n",
       " 619: 'adrs',\n",
       " 620: 'ads',\n",
       " 621: 'adsa',\n",
       " 622: 'adserver',\n",
       " 623: 'adservers',\n",
       " 624: 'adserving',\n",
       " 625: 'adsl',\n",
       " 626: 'adstream',\n",
       " 627: 'adswood',\n",
       " 628: 'adt',\n",
       " 629: 'adtech',\n",
       " 630: 'adtspect',\n",
       " 631: 'adult',\n",
       " 632: 'adulthood',\n",
       " 633: 'adultlocation',\n",
       " 634: 'adults',\n",
       " 635: 'adv',\n",
       " 636: 'advaantageous',\n",
       " 637: 'advance',\n",
       " 638: 'advanced',\n",
       " 639: 'advancement',\n",
       " 640: 'advancements',\n",
       " 641: 'advances',\n",
       " 642: 'advancing',\n",
       " 643: 'advantadge',\n",
       " 644: 'advantage',\n",
       " 645: 'advantaged',\n",
       " 646: 'advantageous',\n",
       " 647: 'advantageousexperience',\n",
       " 648: 'advantageousknowledge',\n",
       " 649: 'advantages',\n",
       " 650: 'advantageus',\n",
       " 651: 'advantagous',\n",
       " 652: 'advent',\n",
       " 653: 'adventitious',\n",
       " 654: 'adventure',\n",
       " 655: 'adventurous',\n",
       " 656: 'adversaries',\n",
       " 657: 'adverse',\n",
       " 658: 'adversely',\n",
       " 659: 'adversity',\n",
       " 660: 'advert',\n",
       " 661: 'adverting',\n",
       " 662: 'advertise',\n",
       " 663: 'advertised',\n",
       " 664: 'advertisement',\n",
       " 665: 'advertisements',\n",
       " 666: 'advertiser',\n",
       " 667: 'advertisers',\n",
       " 668: 'advertises',\n",
       " 669: 'advertisin',\n",
       " 670: 'advertising',\n",
       " 671: 'advertorial',\n",
       " 672: 'advertorials',\n",
       " 673: 'adverts',\n",
       " 674: 'advertsiers',\n",
       " 675: 'advertsing',\n",
       " 676: 'advice',\n",
       " 677: 'advices',\n",
       " 678: 'advisable',\n",
       " 679: 'advise',\n",
       " 680: 'advised',\n",
       " 681: 'adviser',\n",
       " 682: 'advisers',\n",
       " 683: 'advisery',\n",
       " 684: 'advises',\n",
       " 685: 'advising',\n",
       " 686: 'advisor',\n",
       " 687: \"advisor's\",\n",
       " 688: 'advisorlocation',\n",
       " 689: 'advisors',\n",
       " 690: 'advisorsa',\n",
       " 691: 'advisory',\n",
       " 692: 'advocacy',\n",
       " 693: 'advocate',\n",
       " 694: 'advocates',\n",
       " 695: 'advocating',\n",
       " 696: 'adwords',\n",
       " 697: 'ae',\n",
       " 698: 'aec',\n",
       " 699: 'aecc',\n",
       " 700: 'aed',\n",
       " 701: 'aegis',\n",
       " 702: 'aegon',\n",
       " 703: 'ael',\n",
       " 704: 'aer',\n",
       " 705: 'aerial',\n",
       " 706: 'aermod',\n",
       " 707: 'aero',\n",
       " 708: 'aeroconseil',\n",
       " 709: 'aerodynamic',\n",
       " 710: 'aerodynamics',\n",
       " 711: 'aeroengine',\n",
       " 712: 'aeroengines',\n",
       " 713: 'aeroflex',\n",
       " 714: 'aerohive',\n",
       " 715: 'aeromechanical',\n",
       " 716: 'aeronautical',\n",
       " 717: 'aeronautics',\n",
       " 718: 'aeroplane',\n",
       " 719: 'aerosol',\n",
       " 720: 'aerospace',\n",
       " 721: 'aerospacesectors',\n",
       " 722: 'aerostructures',\n",
       " 723: 'aerotek',\n",
       " 724: 'aes',\n",
       " 725: 'aesco',\n",
       " 726: 'aesps',\n",
       " 727: 'aesthetic',\n",
       " 728: 'aesthetically',\n",
       " 729: 'aesthetics',\n",
       " 730: 'aet',\n",
       " 731: 'af',\n",
       " 732: 'afaria',\n",
       " 733: 'afc',\n",
       " 734: 'afcila',\n",
       " 735: 'afdeling',\n",
       " 736: 'afdelingen',\n",
       " 737: 'afe',\n",
       " 738: 'afety',\n",
       " 739: 'affable',\n",
       " 740: 'affair',\n",
       " 741: 'affairs',\n",
       " 742: 'affect',\n",
       " 743: 'affected',\n",
       " 744: 'affecting',\n",
       " 745: 'affective',\n",
       " 746: 'affectively',\n",
       " 747: 'affects',\n",
       " 748: 'affiliate',\n",
       " 749: 'affiliated',\n",
       " 750: 'affiliates',\n",
       " 751: 'affiliation',\n",
       " 752: 'affiliations',\n",
       " 753: 'affinities',\n",
       " 754: 'affinity',\n",
       " 755: 'affirmation',\n",
       " 756: 'affirming',\n",
       " 757: 'affirms',\n",
       " 758: 'affluent',\n",
       " 759: 'afford',\n",
       " 760: 'affordability',\n",
       " 761: 'affordable',\n",
       " 762: 'afforded',\n",
       " 763: 'affording',\n",
       " 764: 'affords',\n",
       " 765: 'afghanistan',\n",
       " 766: 'afi',\n",
       " 767: 'aficionado',\n",
       " 768: 'afid',\n",
       " 769: 'afield',\n",
       " 770: 'afl',\n",
       " 771: 'afm',\n",
       " 772: 'afore',\n",
       " 773: 'aforementioned',\n",
       " 774: 'afpc',\n",
       " 775: 'afpp',\n",
       " 776: 'afraid',\n",
       " 777: 'africa',\n",
       " 778: 'african',\n",
       " 779: 'afrikaans',\n",
       " 780: 'afrl',\n",
       " 781: 'aftercare',\n",
       " 782: 'aftercares',\n",
       " 783: 'aftereffects',\n",
       " 784: 'afterhours',\n",
       " 785: 'aftermarket',\n",
       " 786: 'aftermarkets',\n",
       " 787: 'aftermath',\n",
       " 788: 'afternoon',\n",
       " 789: 'afternoons',\n",
       " 790: 'afters',\n",
       " 791: 'aftersales',\n",
       " 792: 'afterschool',\n",
       " 793: 'afx',\n",
       " 794: 'ag',\n",
       " 795: 'aga',\n",
       " 796: 'agajustsocialcare',\n",
       " 797: 'agatha',\n",
       " 798: 'agc',\n",
       " 799: 'age',\n",
       " 800: 'ageas',\n",
       " 801: 'aged',\n",
       " 802: 'agee',\n",
       " 803: 'ageing',\n",
       " 804: 'agencies',\n",
       " 805: 'agencieslocal',\n",
       " 806: 'agency',\n",
       " 807: \"agency's\",\n",
       " 808: 'agencyadecco',\n",
       " 809: 'agencyclient',\n",
       " 810: 'agencys',\n",
       " 811: 'agencyside',\n",
       " 812: 'agenda',\n",
       " 813: 'agendas',\n",
       " 814: 'agent',\n",
       " 815: 'agentry',\n",
       " 816: 'agents',\n",
       " 817: 'agerelated',\n",
       " 818: 'ages',\n",
       " 819: 'ageto',\n",
       " 820: 'ageukessex',\n",
       " 821: 'aggregate',\n",
       " 822: 'aggregated',\n",
       " 823: 'aggregates',\n",
       " 824: 'aggregation',\n",
       " 825: 'aggregations',\n",
       " 826: 'aggregator',\n",
       " 827: 'aggregators',\n",
       " 828: 'aggression',\n",
       " 829: 'aggressive',\n",
       " 830: 'aggressively',\n",
       " 831: 'aggresso',\n",
       " 832: 'aggs',\n",
       " 833: 'agile',\n",
       " 834: 'agilebased',\n",
       " 835: 'agiledriven',\n",
       " 836: 'agileish',\n",
       " 837: 'agilelondon',\n",
       " 838: 'agilent',\n",
       " 839: 'agileour',\n",
       " 840: 'agility',\n",
       " 841: 'agincare',\n",
       " 842: 'aging',\n",
       " 843: 'agis',\n",
       " 844: 'agitated',\n",
       " 845: 'agm',\n",
       " 846: 'agms',\n",
       " 847: 'agnieszka',\n",
       " 848: 'agnostic',\n",
       " 849: 'ago',\n",
       " 850: 'agostini',\n",
       " 851: 'agree',\n",
       " 852: 'agreeable',\n",
       " 853: 'agreed',\n",
       " 854: \"agreedsla's\",\n",
       " 855: 'agreeing',\n",
       " 856: 'agreement',\n",
       " 857: 'agreements',\n",
       " 858: 'agrees',\n",
       " 859: 'agresso',\n",
       " 860: 'agri',\n",
       " 861: 'agricultural',\n",
       " 862: 'agriculture',\n",
       " 863: 'ags',\n",
       " 864: 'agv',\n",
       " 865: 'agwedd',\n",
       " 866: 'agy',\n",
       " 867: 'ah',\n",
       " 868: 'aharrisonoutsourceuk',\n",
       " 869: 'ahdl',\n",
       " 870: 'ahead',\n",
       " 871: 'aherne',\n",
       " 872: 'ahg',\n",
       " 873: 'ahmadrandstad',\n",
       " 874: 'ahmed',\n",
       " 875: 'ahmedadvantageresourcing',\n",
       " 876: 'ahmeders',\n",
       " 877: 'ahp',\n",
       " 878: 'ahrc',\n",
       " 879: 'ahts',\n",
       " 880: 'ahu',\n",
       " 881: \"ahu's\",\n",
       " 882: 'ahus',\n",
       " 883: 'ahve',\n",
       " 884: 'ahwb',\n",
       " 885: 'ai',\n",
       " 886: 'aia',\n",
       " 887: 'aiag',\n",
       " 888: 'aib',\n",
       " 889: 'aid',\n",
       " 890: 'aidan',\n",
       " 891: 'aide',\n",
       " 892: 'aided',\n",
       " 893: 'aider',\n",
       " 894: 'aiders',\n",
       " 895: 'aiding',\n",
       " 896: 'aids',\n",
       " 897: 'aif',\n",
       " 898: 'aifmd',\n",
       " 899: 'aig',\n",
       " 900: 'aiggre',\n",
       " 901: 'ail',\n",
       " 902: 'aila',\n",
       " 903: 'ailments',\n",
       " 904: 'aim',\n",
       " 905: 'aima',\n",
       " 906: 'aime',\n",
       " 907: 'aimed',\n",
       " 908: 'aimee',\n",
       " 909: 'aiming',\n",
       " 910: 'aims',\n",
       " 911: 'ain',\n",
       " 912: 'ainscoughevolutionjobs',\n",
       " 913: 'aintaining',\n",
       " 914: 'aintenance',\n",
       " 915: 'aintree',\n",
       " 916: 'air',\n",
       " 917: 'airb',\n",
       " 918: 'airborne',\n",
       " 919: 'airbus',\n",
       " 920: 'aircom',\n",
       " 921: \"aircom's\",\n",
       " 922: 'airconditioned',\n",
       " 923: 'airconditioning',\n",
       " 924: 'aircraft',\n",
       " 925: 'aircrafts',\n",
       " 926: 'airdrie',\n",
       " 927: 'airfield',\n",
       " 928: 'airflow',\n",
       " 929: 'airframe',\n",
       " 930: 'airframes',\n",
       " 931: 'airfreight',\n",
       " 932: 'airinterface',\n",
       " 933: 'airline',\n",
       " 934: 'airlines',\n",
       " 935: 'airmonitoring',\n",
       " 936: 'airport',\n",
       " 937: 'airports',\n",
       " 938: 'airrecruitment',\n",
       " 939: 'airsampling',\n",
       " 940: 'airside',\n",
       " 941: 'airtesting',\n",
       " 942: 'airtime',\n",
       " 943: 'airtoair',\n",
       " 944: 'airwatch',\n",
       " 945: 'airway',\n",
       " 946: 'airways',\n",
       " 947: 'airworthiness',\n",
       " 948: 'airy',\n",
       " 949: 'ais',\n",
       " 950: 'aishah',\n",
       " 951: 'ait',\n",
       " 952: 'aix',\n",
       " 953: 'aj',\n",
       " 954: 'aja',\n",
       " 955: 'ajanthan',\n",
       " 956: 'ajax',\n",
       " 957: 'ajay',\n",
       " 958: 'ajaymasstemps',\n",
       " 959: 'ajaz',\n",
       " 960: 'ajc',\n",
       " 961: 'ajilon',\n",
       " 962: 'ajith',\n",
       " 963: 'ajunior',\n",
       " 964: 'aka',\n",
       " 965: 'akamai',\n",
       " 966: 'akbar',\n",
       " 967: 'ake',\n",
       " 968: 'akhtarsecrecruitment',\n",
       " 969: 'akin',\n",
       " 970: 'akka',\n",
       " 971: 'akqa',\n",
       " 972: 'akton',\n",
       " 973: 'al',\n",
       " 974: 'alacarte',\n",
       " 975: 'alainchong',\n",
       " 976: 'alan',\n",
       " 977: 'alana',\n",
       " 978: 'alanfactusrec',\n",
       " 979: 'alansiquotcbsbutler',\n",
       " 980: 'alarm',\n",
       " 981: 'alarming',\n",
       " 982: 'alarms',\n",
       " 983: 'alarp',\n",
       " 984: 'alastair',\n",
       " 985: 'alastairwreubensinclair',\n",
       " 986: 'albans',\n",
       " 987: 'albany',\n",
       " 988: 'albeit',\n",
       " 989: 'albert',\n",
       " 990: 'albion',\n",
       " 991: 'albior',\n",
       " 992: 'alc',\n",
       " 993: 'alcatel',\n",
       " 994: 'alcatellucent',\n",
       " 995: 'alcazar',\n",
       " 996: 'alcester',\n",
       " 997: 'alchemist',\n",
       " 998: 'alchemy',\n",
       " 999: 'alco',\n",
       " ...}"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generates the w_index:word dictionary\n",
    "voc_dict = gen_vocIndex(voc1)\n",
    "voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40038"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the size of vocab dictonary\n",
    "len(voc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking the type of tfidf_features\n",
    "type(tfidf_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reducing the float size from float32 to float16 to overome memory issues.\n",
    "tfidf_features1=tfidf_features.astype(\"float16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<1x40038 sparse matrix of type '<class 'numpy.float16'>'\n",
       "\twith 71 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#displaying a sample from tfidf features.\n",
    "tfidf_features1[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_vectorFile(data_features,filename):\n",
    "    num = data_features.shape[0] # the number of document\n",
    "    out_file = open(filename, 'w') # creates a txt file and open to save the vector representation\n",
    "    for a_ind in range(0, num): # loop through each article by index\n",
    "        for f_ind in data_features[a_ind].nonzero()[1]: # for each word index that has non-zero entry in the data_feature\n",
    "            value = data_features[a_ind][0,f_ind] # retrieve the value of the entry from data_features\n",
    "            out_file.write(\"{}:{} \".format(f_ind,value)) # write the entry to the file in the format of word_index:value\n",
    "        out_file.write('\\n') # start a new line after each article\n",
    "    out_file.close() # close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "tVector_file = \"./jobDesc_tVector.txt\" # file name of the tfidf vector\n",
    "write_vectorFile(tfidf_features,tVector_file) # write the tfidf vector to file. So the tVector file can be used again the below tasks at later stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_wordweights(fName_tVectors, voc_dict):\n",
    "    tfidf_weights = [] # a list to store the  word:weight dictionaries of documents\n",
    "    \n",
    "    with open(fName_tVectors) as tVecf: \n",
    "        tVectors = tVecf.read().splitlines() # each line is a tfidf vector representation of a document in string format 'word_index:weight word_index:weight .......'\n",
    "    for tv in tVectors: # for each tfidf document vector\n",
    "        tv = tv.strip()\n",
    "        weights = tv.split(' ') # list of 'word_index:weight' entries\n",
    "        weights = [w.split(':') for w in weights] # change the format of weight to a list of '[word_index,weight]' entries\n",
    "        wordweight_dict = {voc_dict[int(w[0])]:w[1] for w in weights} # construct the weight dictionary, where each entry is 'word:weight'\n",
    "        tfidf_weights.append(wordweight_dict) \n",
    "    return tfidf_weights\n",
    "\n",
    "fName_tVectors = 'jobDesc_tVector.txt'\n",
    "tfidf_weights = doc_wordweights(fName_tVectors, voc_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "71"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Displaying the length of sample elements\n",
    "len(tfidf_weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extended version of the `gen_docVecs` function-to be used to create weighted tfidf documents vectors\n",
    "def gen_docVecs_weighted(wv,tk_txts,tfidf = []): # generate vector representation for documents\n",
    "    docs_vectors =[] \n",
    "    for i in range(0,len(tk_txts)):\n",
    "        tokens = list(set(tk_txts[i])) # get the list of distinct words of the document\n",
    "        temp=[] # creating a temp list(store value for 1st doc & for 2nd doc remove the details of 1st & proced through 2nd and so on..)\n",
    "        temp2=[] #created a temp2 list to append temp values.\n",
    "        for w_ind in range(0, len(tokens)): # looping through each word of a single document and spliting through space\n",
    "            try:\n",
    "                word = tokens[w_ind]\n",
    "                word_vec = wv[word] # if word is present in embeddings(goole provides weights associate with words(300)) then proceed\n",
    "                \n",
    "                if tfidf != []:\n",
    "                    word_weight = float(tfidf[i][word])\n",
    "                else:\n",
    "                    word_weight = 1\n",
    "                #temp = temp.append(pd.Series(word_vec*word_weight), ignore_index = True) # if word is present then append it to temporary dataframe\n",
    "                temp = pd.Series(word_vec*word_weight)\n",
    "                temp2.append(temp)\n",
    "            except:\n",
    "                pass\n",
    "            temp=[]\n",
    "        zk=np.sum(np.array(temp2),axis=0) #adding the array values across axis 0-running vertically dowards across rows.\n",
    "        docs_vectors.append(zk) #\n",
    "    return docs_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generated a weighted tfidf document vector for glove word vectors\n",
    "weighted_jobDesc_glove_dvs = gen_docVecs_weighted(glove_wv,job_desc_tokens,tfidf_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55449, 200)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_jobDesc_glove_dvs_arr=np.array(weighted_jobDesc_glove_dvs) #Converting the document vectors into arrays.\n",
    "weighted_jobDesc_glove_dvs_arr.shape #Displaying the size of array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generated a weighted tfidf document vector for fastText word vectors\n",
    "weighted_jobDescFT_dvs = gen_docVecs_weighted(jobDescFT_wv,job_desc_tokens,tfidf_weights) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55449, 200)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_jobDescFT_dvs_arr=np.array(weighted_jobDescFT_dvs) #Converting the document vectors into arrays.\n",
    "weighted_jobDescFT_dvs_arr.shape #Displaying the size of array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generated a weighted tfidf document vector for google-300-news word vectors\n",
    "weighted_jobDesc_g300 = gen_docVecs_weighted(preTW2v_g300_wv,job_desc_tokens,tfidf_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55449, 300)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weighted_jobDesc_g300_arr=np.array(weighted_jobDesc_g300) #Converting the document vectors into arrays.\n",
    "weighted_jobDesc_g300_arr.shape #Displaying the size of array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving outputs\n",
    "Save the count vector representation as per spectification.\n",
    "- count_vectors.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save output data...\n",
    "#creating count_vectors.txt file as per the required format.\n",
    "with open('count_vectors.txt', 'w') as f:\n",
    "    for i,j in zip(count_vec,webindex) :\n",
    "        ss=\"#\"+str(j)+\",\"+i\n",
    "        f.write(\"%s\\n\" % ss)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3. Job Advertisement Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...... Sections and code blocks on buidling classification models based on different document feature represetations. \n",
    "Detailed comparsions and evaluations on different models to answer each question as per specification. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models=[]  #list to store the classifiation model name\n",
    "list_models_score=[] #List to store the accuracy score of the respective model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8763450556056508"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Created a logistic regression model with max_iterations of 1000 and solver as default-ibfgs and passing count vectors and categories\n",
    "seed=0 #seed is declared and initialised.\n",
    "# creating training and test split\n",
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split(count_features, category12, list(range(0,len(category12))),test_size=0.30, random_state=seed)\n",
    "jobDescv_LRmodel = LogisticRegression(max_iter = 1000,random_state=seed) #logistic regression model is built..\n",
    "jobDescv_LRmodel.fit(X_train, y_train)\n",
    "temp=jobDescv_LRmodel.score(X_test, y_test) #accuracy of count vectors model is extracted\n",
    "temp  #Displaying the accuracy score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfk=pd.read_csv(\"jobDescFT_dvs.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kk1=np.array(dfk) #Converting the document vectors into arrays.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55449, 200)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kk1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "jobDescFT_dvs_arr=kk1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models.append(\"Count Vectors LR model\") #stored model name\n",
    "list_models_score.append(temp) #stored model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "pkl_filn=\"jobDescFT.pkl\"\n",
    "with open(pkl_filn,'wb') as file:\n",
    "    pickle.dump(jobDesFT_LRmodel,file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8658851818455064"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Created a logistic regression model with max_iterations of 6000 and solver as default-ibfgs and passing unweighted fasttext doc vectors and categories\n",
    "seed = 0 #seed is declared and initialised.\n",
    "# creating training and test split\n",
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split(jobDescFT_dvs_arr, category12, list(range(0,len(category12))),test_size=0.30, random_state=seed)\n",
    "jobDesFT_LRmodel = LogisticRegression(max_iter = 6000,random_state=seed)\n",
    "jobDesFT_LRmodel.fit(X_train, y_train)\n",
    "temp=jobDesFT_LRmodel.score(X_test, y_test) #accuracy of unweighted fasttext model is extracted\n",
    "temp  #displaying score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models.append(\"FastText unweighted LR model\") #stored model name\n",
    "list_models_score.append(temp) #stored model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8696122633002705"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Created a logistic regression model with max_iterations of 5000 and solver as default-ibfgs and passing weighted fasttext doc vectors and categories\n",
    "seed = 0 #seed is declared and initialzed. It will be used as random state in logistic regression model.\n",
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split(weighted_jobDescFT_dvs_arr, category12, list(range(0,len(category12))),test_size=0.30, random_state=seed)\n",
    "# creating training and test split\n",
    "jobDesFT_weighted_LRmodel = LogisticRegression(max_iter = 5000,random_state=seed)\n",
    "jobDesFT_weighted_LRmodel.fit(X_train, y_train)\n",
    "temp=jobDesFT_weighted_LRmodel.score(X_test, y_test) #accuracy of weighted fasttext model is extracted\n",
    "temp #displaying score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models.append(\"FastText weighted tfidf LR model\") #stored model name\n",
    "list_models_score.append(temp) #stored model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8592125037571385"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Created a logistic regression model with max_iterations of 5000 and solver as default-ibfgs and passing unweighted google-news-300 doc vectors and categories\n",
    "seed = 0 #seed is declared and initialzed. It will be used as random state in logistic regression model.\n",
    "# creating training and test split\n",
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split(jobDesc_g300_dvs_arr, category12, list(range(0,len(category12))),test_size=0.30, random_state=seed)\n",
    "jobDesg300_LRmodel = LogisticRegression(max_iter = 5000,random_state=seed)\n",
    "jobDesg300_LRmodel.fit(X_train, y_train)\n",
    "temp=jobDesg300_LRmodel.score(X_test, y_test) #accuracy of unweighted google 300 news model is extracted\n",
    "temp #displaying score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models.append(\"Pre trained google-300-news unweighted LR model\") #stored model name\n",
    "list_models_score.append(temp)  #stored model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8651036970243463"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Created a logistic regression model with max_iterations of 5000 and solver as default-ibfgs and passing weighted google-news-300 tfidf doc vectors and categories\n",
    "seed = 0 #seed is declared and initialzed. It will be used as random state in logistic regression model.\n",
    "# creating training and test split\n",
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split(weighted_jobDesc_g300_arr, category12, list(range(0,len(category12))),test_size=0.30, random_state=seed)\n",
    "jobDesg300_weighted_LRmodel = LogisticRegression(max_iter = 5000,random_state=seed)\n",
    "jobDesg300_weighted_LRmodel.fit(X_train, y_train)\n",
    "temp=jobDesg300_weighted_LRmodel.score(X_test, y_test) #accuracy of weighted google 300 news model is extracted\n",
    "temp #displaying score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models.append(\"Pre trained google-300-news weighted tfidf LR model\") #stored model name\n",
    "list_models_score.append(temp) #stored model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8508566275924256"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Created a logistic regression model with max_iterations of 5000 and solver as default-ibfgs and passing unweighted glove doc vectors and categories\n",
    "seed = 0 #seed is declared and initialzed. It will be used as random state in logistic regression model.\n",
    "# creating training and test split\n",
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split(jobDesc_glove_dvs_arr, category12, list(range(0,len(category12))),test_size=0.30, random_state=seed)\n",
    "jobDesglove_LRmodel = LogisticRegression(max_iter = 5000,random_state=seed)\n",
    "jobDesglove_LRmodel.fit(X_train, y_train)\n",
    "temp=jobDesglove_LRmodel.score(X_test, y_test) #accuracy of unweighted glove model is extracted\n",
    "temp #displaying score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models.append(\"Pre trained glove unweighted LR model\") #stored model name\n",
    "list_models_score.append(temp) #stored model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8515779981965735"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Created a logistic regression model with max_iterations of 5000 and solver as default-ibfgs and passing weighted tfidf glove doc vectors and categories\n",
    "seed = 0 #seed is declared and initialzed. It will be used as random state in logistic regression model.\n",
    "# creating training and test split\n",
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split(weighted_jobDesc_glove_dvs_arr, category12, list(range(0,len(category12))),test_size=0.30, random_state=seed)\n",
    "jobDesglove_weighted_LRmodel = LogisticRegression(max_iter = 5000,random_state=seed)\n",
    "jobDesglove_weighted_LRmodel.fit(X_train, y_train)\n",
    "temp=jobDesglove_weighted_LRmodel.score(X_test, y_test) #accuracy of weighted glove model is extracted\n",
    "temp #displaying score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_models.append(\"Pre trained glove weighted tfidf LR model\")  #stored model name\n",
    "list_models_score.append(temp) #stored model score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classification model                                                  Model score\n",
      "Count Vectors LR model                                                0.8763450556056508\n",
      "FastText unweighted LR model                                          0.8659452960625188\n",
      "FastText weighted tfidf LR model                                      0.8696122633002705\n",
      "FastText unweighted LR model                                          0.8592125037571385\n",
      "FastText weighted tfidf LR model                                      0.8651036970243463\n",
      "Pre trained google-300-news unweighted LR model                       0.8508566275924256\n",
      "Pre trained google-300-news weighted tfidf LR model                   0.8515779981965735\n"
     ]
    }
   ],
   "source": [
    "print(\"Classification model\"+ \" \"*50 + \"Model score\")\n",
    "for i,j in zip(list_models,list_models_score):\n",
    "    print(i.ljust(70)+str(j))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on above model scores, we can conclude that count vector logistic regression model score (0.876) is higher than other models. And only Fastext weighted model got closer score(0.869).\n",
    "\n",
    "Also, all models creted using the weighted document vectores performed much better than the unweighted ones.\n",
    "\n",
    "In terms of model performance, count vector model beats the other models in terms of accuracy scores.\n",
    "\n",
    "Pretrained models, google300-news and glove did not perform well. As fasttext and counter vector generated word vectors based on the inputted datasets whereas the pre trained word vectors are much generalised. Hence, we observe this disparity in pre trained vs models trained on the current data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building Logistic regression models of Job title+description and job title fasttext word embeddings. and comparing that with the extracted logistic regression model based on job description data alone.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=23115, vector_size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Code to perform the task...\n",
    "#Creating a fast text model based on job Title + description\n",
    "# 1. Set the corpus file names/path\n",
    "corpus_file = './job_title_descrip1.txt'\n",
    "\n",
    "# 2. Initialise the Fast Text model\n",
    "jobTitleDescFT = FastText(vector_size=200) \n",
    "\n",
    "# 3. build the vocabulary\n",
    "jobTitleDescFT.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "# 4. train the model\n",
    "jobTitleDescFT.train(\n",
    "    corpus_file=corpus_file, epochs=jobTitleDescFT.epochs,\n",
    "    total_examples=jobTitleDescFT.corpus_count, total_words=jobTitleDescFT.corpus_total_words,\n",
    ")\n",
    "\n",
    "print(jobTitleDescFT) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastTextKeyedVectors object at 0x000001AC87324BE0>\n"
     ]
    }
   ],
   "source": [
    "jobTitleDescFT_wv = jobTitleDescFT.wv  #extracted word vectors based on the job title+description data.\n",
    "print(jobTitleDescFT_wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55449, 200)\n"
     ]
    }
   ],
   "source": [
    "# NOTE this can take some time to finish running\n",
    "# generate docum   ent embeddings\n",
    "jobTitleDescFT_dvs12 = gen_docVecs_modified1(jobTitleDescFT_wv,job_title_desc_tokens) #extracting the unweighted fasttext doc vectors for job title+description data.\n",
    "jobTitleDesc_dvs_arr=np.array(jobTitleDescFT_dvs12) #converting extracted doc vectors into arrays.\n",
    "print(jobTitleDesc_dvs_arr.shape)  #displaying the shape of array.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8694319206492336"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#modelling based on unweighted fasttext- job title+description\n",
    "#Created a logistic regression model with max_iterations of 6000 and solver as default-ibfgs and passing unweighted fasttext doc vectors of job title+description and categories\n",
    "seed = 0 #seed is declared and initialzed. It will be used as random state in logistic regression model.\n",
    "# creating training and test split\n",
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split(jobTitleDesc_dvs_arr,category12, list(range(0,len(category12))),test_size=0.30, random_state=seed)\n",
    "jobTitleDescFT_LRmodel = LogisticRegression(max_iter = 6000,random_state=seed)\n",
    "jobTitleDescFT_LRmodel.fit(X_train, y_train)\n",
    "temp1=jobTitleDescFT_LRmodel.score(X_test, y_test) #accuracy of unweighted fasttext model is extracted\n",
    "temp1 #displaying the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FastText(vocab=3097, vector_size=200, alpha=0.025)\n"
     ]
    }
   ],
   "source": [
    "# Code to perform the task...\n",
    "#Creating a fast text model based on job Title \n",
    "# 1. Set the corpus file names/path\n",
    "corpus_file = './job_title1.txt'\n",
    "\n",
    "# 2. Initialise the Fast Text model\n",
    "jobTitleFT = FastText(vector_size=200) \n",
    "\n",
    "# 3. build the vocabulary\n",
    "jobTitleFT.build_vocab(corpus_file=corpus_file)\n",
    "\n",
    "# 4. train the model\n",
    "jobTitleFT.train(\n",
    "    corpus_file=corpus_file, epochs=jobTitleFT.epochs,\n",
    "    total_examples=jobTitleFT.corpus_count, total_words=jobTitleFT.corpus_total_words,\n",
    ")\n",
    "\n",
    "print(jobTitleFT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<gensim.models.fasttext.FastTextKeyedVectors object at 0x000001AC80A67400>\n"
     ]
    }
   ],
   "source": [
    "jobTitleFT_wv = jobTitleFT.wv  #Extracted word vectors based on the job title data.\n",
    "print(jobTitleFT_wv) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(55449, 200)\n"
     ]
    }
   ],
   "source": [
    "# NOTE this can take some time to finish running\n",
    "# generated document embeddings\n",
    "jobTitleFT_dvs12 = gen_docVecs_modified1(jobTitleFT_wv,job_title_tokens)  #extracted document vectors.\n",
    "jobTitleFT_dvs_arr=np.array(jobTitleFT_dvs12) #the document vectors are converted into array\n",
    "print(jobTitleFT_dvs_arr.shape) #Displaying the array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.34445446348061315"
      ]
     },
     "execution_count": 124,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#modelling based on unweighted fasttext\n",
    "#Created a logistic regression model with max_iterations of 1000 and solver as default-ibfgs and passing unweighted fasttext doc vectors of job title and categories\n",
    "seed = 0 #seed is declared and initialzed. It will be used as random state in logistic regression model.\n",
    "# creating training and test split\n",
    "X_train, X_test, y_train, y_test,train_indices,test_indices = train_test_split(jobTitleFT_dvs_arr,category12, list(range(0,len(category12))),test_size=0.30, random_state=seed)\n",
    "jobTitleFT_LRmodel = LogisticRegression(max_iter = 1000,random_state=seed)\n",
    "jobTitleFT_LRmodel.fit(X_train, y_train)\n",
    "temp2=jobTitleFT_LRmodel.score(X_test, y_test)#accuracy of unweighted fastext logistic regression model is extracted\n",
    "temp2 #displaying score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data used to build model                    model score\n",
      " Job description                        0.8659452960625188\n",
      " Job Title+ description                 0.8694319206492336\n",
      " Job Title                              0.34445446348061315\n"
     ]
    }
   ],
   "source": [
    "print(\"Data used to build model\"+\" \"*20+\"model score\")\n",
    "print(\" Job description\".ljust(40)+str(list_models_score[1]))\n",
    "print(\" Job Title+ description\".ljust(40)+str(temp1))\n",
    "print(\" Job Title\".ljust(40)+str(temp2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the above classification model score, we can confirm that the model trained on job title+description got more accuracy score (0.869) than the model based on job description data (0.865).\n",
    "\n",
    "Thus, we can conclude that more amount of data can improve the model performance. But on downside too much amount of data also leads to usage of high compuatation resources.\n",
    "\n",
    "Also, the worse performing model is the model biult by using job title data alone (0.344). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "#The approximate time it takes to run task 2 and 3 is 140-160 minutes."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
